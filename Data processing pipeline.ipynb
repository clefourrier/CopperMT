{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c005eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "840378de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26cedf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding module to sys path\n",
    "import sys\n",
    "sys.path.append(\"/home/cfourrie/documents/software/public/CopperMT/\")\n",
    "# RNN imports\n",
    "import pipeline\n",
    "import torch, numpy as np\n",
    "from fairseq import checkpoint_utils, data, options, tasks\n",
    "from pipeline.neural_translation.multilingual_rnns.multilingual_rnn import MultilingualRNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b414a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "copper_dir = \"/home/cfourrie/documents/software/public/CopperMT/\"\n",
    "raw_data_path = \"inputs/raw_data/\" \n",
    "split_data_path = \"inputs/split_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b84c2256",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"abrahammonpa\", \"allenbai\", \"backstromnorthernpakistan\", \"castrosui\", \"davletshinaztecan\", \n",
    "           \"felekesemitic\", \"hantganbangime\", \"hattorijaponic\", \"listsamplesize\", \"mannburmish\"]\n",
    "splits = [\"0.10\", \"0.20\", \"0.30\", \"0.40\", \"0.50\"]\n",
    "models = [\"baseline\", \"BiNMT\", \"MNMT\", \"SMT\"]\n",
    "train_name = \"training\"\n",
    "test_in_name = \"test\"\n",
    "test_out_name = \"solutions\"\n",
    "save_as = \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ddbf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_needed = False\n",
    "preprocessing_needed = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af2e2cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"/home/cfourrie/documents/software/public/CopperMT/workspace\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43463273",
   "metadata": {},
   "outputs": [],
   "source": [
    "long2short_castrosui = {'AntangWesternSandong': 'Antang', 'BanliangYangAn': 'Banliang', 'DujiangEasternSandong': 'Dujiang', 'JiaoliPandong': 'Jiaoli', 'JiarongSouthernSandong': 'Jiarong', 'JiuqianSouthernSandong': 'Jiuqian', 'Pandong': 'Pandong', 'RenliEasternSandong': 'Renli', 'SanjiangEasternSandong': 'Sanjiang', 'ShuigenCentralSandong': 'Shuigen', 'ShuiweiSouthernSandong': 'Shuiwei', 'ShuiyaoSouthernSandong': 'Shuiyao', 'TangnianYangAn': 'Tangnian', 'TangzhouWesternSandong': 'Tangzhou', 'TingpaiWesternSandong': 'Tingpai', 'ZhongheCentralSandong': 'Zhonghe'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26858afc",
   "metadata": {},
   "source": [
    "## Convert data to usual format for our software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef4e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you do not want to share embeddings, removed shared_ from the folder path\n",
    "if conversion_needed:\n",
    "    for folder in folders:\n",
    "        for split in splits:\n",
    "            try: os.makedirs(os.path.join(split_data_path, f\"shared_{folder}\", split))\n",
    "            except FileExistsError: pass\n",
    "            # Training\n",
    "            cur_data = pd.read_csv(os.path.join(raw_data_path, folder, f\"training-{split}.tsv\"), sep=\"\\t\")\n",
    "            cur_languages = [c for c in cur_data.columns if c != 'COGID']\n",
    "            if folder == \"castrosui\":\n",
    "                cur_languages = [re.split('(?=[A-Z])', l)[1] for l in cur_languages]\n",
    "                cur_data.columns = ['COGID'] + cur_languages\n",
    "            for l1, l2 in itertools.combinations(cur_languages, 2):\n",
    "                for name in [f\"{l1}-{l2}\", f\"{l2}-{l1}\"]:\n",
    "                    with open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"train.{name}.{l1}\"), \"w+\") as f1, \\\n",
    "                         open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"train.{name}.{l2}\"), \"w+\") as f2:\n",
    "                        for ix, row in cur_data[[l1, l2]].dropna().iterrows():\n",
    "                            f1.write(row[l1] + \"\\n\")\n",
    "                            f2.write(row[l2] + \"\\n\")\n",
    "            for l in cur_languages:\n",
    "                with open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"train.{l}-{l}.{l}\"), \"w+\") as f:\n",
    "                    for ix, row in cur_data[[l]].dropna().iterrows():\n",
    "                        f.write(row[l] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6855a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of test and validation sets \n",
    "# If you do not want to share embeddings, removed shared_ from the folder path\n",
    "if conversion_needed:\n",
    "    for folder in folders:\n",
    "        for split in splits:\n",
    "            try: os.makedirs(os.path.join(split_data_path, f\"shared_{folder}\", split))\n",
    "            except FileExistsError: pass\n",
    "            # Training\n",
    "            cur_data = pd.read_csv(os.path.join(raw_data_path, folder, f\"test-{split}.tsv\"), sep=\"\\t\")\n",
    "            cur_languages = [c for c in cur_data.columns if c != 'COGID']\n",
    "            if folder == \"castrosui\":\n",
    "                cur_languages = [re.split('(?=[A-Z])', l)[1] for l in cur_languages]\n",
    "                cur_data.columns = ['COGID'] + cur_languages\n",
    "\n",
    "            for l1, l2 in itertools.combinations(cur_languages, 2):\n",
    "                with open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"test.{l1}-{l2}.{l1}\"), \"w+\") as f1_test, \\\n",
    "                     open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"test.{l2}-{l1}.{l2}\"), \"w+\") as f2_test, \\\n",
    "                     open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"valid.{l1}-{l2}.{l1}\"), \"w+\") as f1_val, \\\n",
    "                     open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"valid.{l1}-{l2}.{l2}\"), \"w+\") as f2_val, \\\n",
    "                     open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"valid.{l2}-{l1}.{l1}\"), \"w+\") as f1_val_r, \\\n",
    "                     open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"valid.{l2}-{l1}.{l2}\"), \"w+\") as f2_val_r:\n",
    "                        for ix, row in cur_data[[l1, l2]].dropna().iterrows():\n",
    "                            if row[l1] != \"?\" and row[l2] != \"?\":                                \n",
    "                                f1_val.write(row[l1] + \"\\n\")                            \n",
    "                                f1_val_r.write(row[l1] + \"\\n\")                            \n",
    "                                f2_val.write(row[l2] + \"\\n\")                            \n",
    "                                f2_val_r.write(row[l2] + \"\\n\")\n",
    "                            if row[l1] == \"?\" and row[l2] != \"?\":\n",
    "                                f2_test.write(row[l2] + \"\\n\")\n",
    "                            if row[l2] == \"?\" and row[l1] != \"?\":\n",
    "                                f1_test.write(row[l1] + \"\\n\")\n",
    "\n",
    "            for l in cur_languages:\n",
    "                with open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"valid.{l}-{l}.{l}\"), \"w+\") as f:\n",
    "                    for ix, row in cur_data[[l]].dropna().iterrows():\n",
    "                        if row[l] != \"?\":\n",
    "                            f.write(row[l] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e05e28",
   "metadata": {},
   "source": [
    "## Generating configuration files contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d7b2426",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if preprocessing_needed:\n",
    "    folder = folders[3]\n",
    "    cur_data = pd.read_csv(os.path.join(raw_data_path, folder, f\"cognates.tsv\"), sep=\"\\t\")\n",
    "    cur_languages = [c for c in cur_data.columns if c != 'COGID']\n",
    "    if folder == \"castrosui\":\n",
    "        cur_languages = [re.split('(?=[A-Z])', l)[1] for l in cur_languages]\n",
    "\n",
    "    print('PROJ_DIR=\"/home/cfourrie/documents/software/CopperMT\"')\n",
    "    print('MOSES_DIR=\"${PROJ_DIR}/submodules\"')\n",
    "    print()\n",
    "    print('WK_DIR=\"${PROJ_DIR}/workspace/' + folder + '\"')\n",
    "    print('INPUTS_DIR=\"${PROJ_DIR}/inputs\"')\n",
    "    print()\n",
    "    print(f'DATA_NAME=\"{folder}\"')\n",
    "\n",
    "    print(f'langs_bi=\"{\",\".join(\"-\".join(l) for l in itertools.product(cur_languages, cur_languages))}\"')\n",
    "    print(f'langs=\"{\",\".join(cur_languages)}\"')\n",
    "    print(f'langs_shared=\"{\"-\".join(cur_languages)}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf8738",
   "metadata": {},
   "source": [
    "Step: 1h30 - In the pipeline folder, execute:\n",
    "\n",
    "`\n",
    "bash data_preprocess.sh parameters_abrahammonpa.cfg\n",
    "bash data_preprocess.sh parameters_allenbai.cfg\n",
    "bash data_preprocess.sh parameters_backstromnorthernpakistan.cfg\n",
    "bash data_preprocess.sh parameters_castrosui.cfg\n",
    "bash data_preprocess.sh parameters_davletshinaztecan.cfg\n",
    "bash data_preprocess.sh parameters_felekesemitic.cfg\n",
    "bash data_preprocess.sh parameters_hantganbangime.cfg\n",
    "bash data_preprocess.sh parameters_hattorijaponic.cfg\n",
    "bash data_preprocess.sh parameters_listsamplesize.cfg\n",
    "bash data_preprocess.sh parameters_mannburmish.cfg\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a787af",
   "metadata": {},
   "source": [
    "## Training models\n",
    "\n",
    "In the pipeline folder, train bilingual neural models with:\n",
    "\n",
    "`\n",
    "bash main_nmt_bilingual_full.sh parameters_abrahammonpa.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_allenbai.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_backstromnorthernpakistan.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_castrosui.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_davletshinaztecan.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_felekesemitic.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_hantganbangime.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_hattorijaponic.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_listsamplesize.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_mannburmish.cfg\n",
    "`\n",
    "\n",
    "In the pipeline folder, train multilingual neural models with:\n",
    "\n",
    "`\n",
    "bash main_nmt_multilingual_full.sh parameters_abrahammonpa.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_allenbai.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_backstromnorthernpakistan.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_castrosui.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_davletshinaztecan.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_felekesemitic.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_hantganbangime.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_hattorijaponic.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_listsamplesize.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_mannburmish.cfg\n",
    "`\n",
    "\n",
    "\n",
    "In the pipeline folder, train bilingual statistical models with:\n",
    "\n",
    "`\n",
    "bash main_smt_full.sh parameters_abrahammonpa.cfg\n",
    "bash main_smt_full.sh parameters_allenbai.cfg\n",
    "bash main_smt_full.sh parameters_backstromnorthernpakistan.cfg\n",
    "bash main_smt_full.sh parameters_castrosui.cfg\n",
    "bash main_smt_full.sh parameters_davletshinaztecan.cfg\n",
    "bash main_smt_full.sh parameters_felekesemitic.cfg\n",
    "bash main_smt_full.sh parameters_hantganbangime.cfg\n",
    "bash main_smt_full.sh parameters_hattorijaponic.cfg\n",
    "bash main_smt_full.sh parameters_listsamplesize.cfg\n",
    "bash main_smt_full.sh parameters_mannburmish.cfg\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4299500",
   "metadata": {},
   "source": [
    "# Choosing best answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b2ed5",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b163463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sublist_with_unk(ls_with_unk, ls):\n",
    "    ls = \"\".join(ls)\n",
    "    ls_with_unk = \"\".join(ls_with_unk)\n",
    "    \n",
    "    for item in ls_with_unk.split(\"<unk>\"):\n",
    "        if item not in ls:\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "260ea9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neural_bleu_predictions(path, l_in, l_out, n_best):\n",
    "    # Storage\n",
    "    source = []\n",
    "    target = []\n",
    "    prediction = []\n",
    "    confidence = []\n",
    "    cur_prediction = []\n",
    "    cur_confidence = []\n",
    "    indices = []\n",
    "    with open(\n",
    "            f'{path}/bleu/bleu_checkpoint_best_{l_in}-{l_out}.{l_out}', 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            line = line.split(\"\\t\")\n",
    "            # Actual source\n",
    "            if \"S-\" in line[0]:\n",
    "                word = line[1].strip(' ').split()\n",
    "                source.append(word)\n",
    "                indices.append(line[0])\n",
    "                # We reinitialize the cur_prediction list\n",
    "                if len(cur_prediction) > 0:\n",
    "                    prediction.append(cur_prediction)\n",
    "                    confidence.append(cur_confidence)\n",
    "                    cur_prediction = []\n",
    "                    cur_confidence = []\n",
    "            # Actual target\n",
    "            if \"T-\" in line[0]:\n",
    "                word = line[1].strip(' ').split()\n",
    "                target.append(word)\n",
    "            # Hypothesis\n",
    "            if \"H-\" in line[0] and len(cur_prediction) < n_best:\n",
    "                word = line[2].strip(' ').split()\n",
    "                cur_prediction.append(word)\n",
    "                cur_confidence.append(math.exp(float(line[1])))\n",
    "        prediction.append(cur_prediction)\n",
    "        confidence.append(cur_confidence)\n",
    "        try:\n",
    "            prediction = [[bor[n] for bor in prediction] for n in range(n_best)]\n",
    "        except IndexError as e:\n",
    "            raise e\n",
    "\n",
    "        #prediction = [[bor[n] for bor in prediction] for n in range(n_best)]\n",
    "\n",
    "    return source, target, prediction, confidence, indices\n",
    "\n",
    "\n",
    "def get_statistical_bleu_predictions(path_data, path, l_in, l_out, n_best, cur_n_best):\n",
    "    target = []\n",
    "    try:\n",
    "        with open(f'{path_data}/test.{l_in}-{l_out}.{l_out}', 'r') as file:\n",
    "            for i, line in enumerate(file):\n",
    "                target.append(line.split())\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    source = []\n",
    "    with open(f'{path_data}/test.{l_in}-{l_out}.{l_in}', 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            source.append(line.split())\n",
    "\n",
    "    prediction = []\n",
    "    confidence = []\n",
    "    cur_ix = -1\n",
    "    cur_prediction = []\n",
    "    cur_confidence = []\n",
    "    indices = []\n",
    "    with open(f'{path}/{l_in}-{l_out}/out/'\n",
    "              f'test.{l_in}-{l_out}_nbest_{str(n_best)}.{l_out}', 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            line = line.split(\"|||\")\n",
    "            ix = int(line[0])\n",
    "            word = line[1].strip(' ').split()\n",
    "\n",
    "            if cur_ix != ix:\n",
    "                if cur_ix != -1:\n",
    "                    indices.append(cur_ix)\n",
    "                    while len(cur_prediction) < cur_n_best:\n",
    "                        cur_prediction.append(cur_prediction[-1])\n",
    "                        cur_confidence.append(cur_confidence[-1])\n",
    "                    prediction.append(cur_prediction)\n",
    "                    confidence.append(cur_confidence)\n",
    "                cur_prediction = [word]\n",
    "                cur_confidence = [math.exp(float(line[-1]))]\n",
    "                cur_ix = ix\n",
    "            else:\n",
    "                cur_prediction.append(word)\n",
    "                cur_confidence.append(math.exp(float(line[-1])))\n",
    "        # Management of last prediction\n",
    "        indices.append(cur_ix)\n",
    "        while len(cur_prediction) < cur_n_best:\n",
    "            cur_prediction.append(cur_prediction[-1])\n",
    "            cur_confidence.append(cur_confidence[-1])\n",
    "        prediction.append(cur_prediction)\n",
    "        confidence.append(cur_confidence)\n",
    "\n",
    "    prediction = [[bor[n] for bor in prediction] for n in range(cur_n_best)]\n",
    "\n",
    "    return source, target, prediction, confidence, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226caa2c",
   "metadata": {},
   "source": [
    "## Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1fa3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_file(langs, model, cur_data):\n",
    "    cur_results_grouped = {lang: defaultdict(dict) for lang in langs}\n",
    "    cur_results_by_lang = {lang: defaultdict(dict) for lang in langs}\n",
    "    # Read results\n",
    "    for lang_out in langs:\n",
    "        for lang_in in langs:\n",
    "            if lang_in == lang_out: continue\n",
    "            tmp_in = long2short_castrosui[lang_in] if cur_data == \"castrosui\" else lang_in\n",
    "            tmp_out = long2short_castrosui[lang_out] if cur_data == \"castrosui\" else lang_out\n",
    "            if model == \"shared_bilingual\":\n",
    "                all_sources, _, all_predictions, all_confidences, all_indices = get_neural_bleu_predictions(\n",
    "                    f\"{models_dir}/{cur_data}/{model}/{tmp_in}-{tmp_out}/{split}\", tmp_in, tmp_out, 10)\n",
    "            elif model == \"shared_multilingual\":\n",
    "                all_sources, _, all_predictions, all_confidences, all_indices = get_neural_bleu_predictions(\n",
    "                    f\"{models_dir}/{cur_data}/{model}/{split}\", tmp_in, tmp_out, 10)\n",
    "            else:\n",
    "                all_sources, _, all_predictions, all_confidences, all_indices = get_statistical_bleu_predictions(\n",
    "                    f\"{split_data_path}/shared_{cur_data}/{split}\",\n",
    "                    f\"{models_dir}/{cur_data}/{model}/{split}\", tmp_in, tmp_out, 10, 10)\n",
    "             \n",
    "            for ix, (source, index) in enumerate(zip(all_sources, all_indices)):\n",
    "                predictions = [\" \".join(all_predictions[n_best][ix]) for n_best in range(10)]\n",
    "                confidences = all_confidences[ix]\n",
    "                cur_results_grouped[lang_out][index].update(\n",
    "                    {f\"{lang_in}_source\": \" \".join(source),\n",
    "                     lang_in: sorted([(p, c) for p, c in zip(predictions, confidences)])}\n",
    "                ) \n",
    "                cur_results_by_lang[lang_out][lang_in].update(\n",
    "                     {\" \".join(source): sorted([(p, c) for p, c in zip(predictions, confidences)])}\n",
    "                ) \n",
    "                \n",
    "                \n",
    "    return cur_results_grouped, cur_results_by_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b7a719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_prediction(row_results, is_stat=True):\n",
    "    predictions_scores = defaultdict(int)\n",
    "    for lang_res in row_results.values():\n",
    "        for pred, score in lang_res:\n",
    "            predictions_scores[pred] += score if is_stat else 1\n",
    "            #predictions_counts[pred] += 1\n",
    "    # prediction scores is better for SMT models! (considerably)\n",
    "    best_prediction = [k for k, v in predictions_scores.items() if v == max(predictions_scores.values())]\n",
    "    if best_prediction:\n",
    "        best_prediction = best_prediction[0]\n",
    "    else:\n",
    "        best_prediction = \"\"\n",
    "\n",
    "    return best_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "529ae67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reordering(raw_data_path, cur_data, model, split, results_by_lang):\n",
    "    test_df = pd.read_csv(os.path.join(raw_data_path, cur_data, f\"test-{split}.tsv\"), sep=\"\\t\")\n",
    "    final_results = defaultdict(list)\n",
    "    for ix, (_, row) in enumerate(test_df.iterrows()):\n",
    "        row_dict = dict(row)\n",
    "        final_results[\"COGID\"].append(row_dict.pop(\"COGID\"))\n",
    "        lang_out = [k for k, v in row_dict.items() if v == \"?\"][0]\n",
    "        row_results = {}\n",
    "        # Compute all predictions\n",
    "        for lang, val in row_dict.items():\n",
    "            if lang == lang_out: continue\n",
    "                \n",
    "            final_results[lang].append(\"\")                \n",
    "            if not isinstance(val, str): continue # nan because was empty\n",
    "            try:\n",
    "                row_results[lang] = results_by_lang[model][lang_out][lang][val]\n",
    "            except KeyError: # some chars are only present in test, and encoded as unk\n",
    "                # We extract possible keys \n",
    "                keys_with_unk = [v for v in results_by_lang[model][lang_out][lang].keys() \n",
    "                                 if \"<unk>\" in v and len(v.split(\" \")) == len(val.split(\" \"))]\n",
    "                possible_keys = []\n",
    "                for key in keys_with_unk:\n",
    "                    if sublist_with_unk(key.split(\" \"), val.split(\" \")):\n",
    "                        possible_keys.append(key)\n",
    "\n",
    "                if len(possible_keys) > 1:\n",
    "                    print(\"Problem! Several plausible keys!\", val, possible_keys)                    \n",
    "                    print(\"Plausible keys chosen is \", possible_keys[0])\n",
    "                elif len(possible_keys) == 0:\n",
    "                    raise Exception(\"Problem! No plausible key!\", val, lang, lang_out)\n",
    "                else:\n",
    "                    row_results[lang] = results_by_lang[model][lang_out][lang][possible_keys[0]]\n",
    "                    \n",
    "            # We filter on length ratio\n",
    "            row_results[lang] = [v for v in row_results[lang] if 0.3 < len(v[0])/len(val) < 3]\n",
    "\n",
    "\n",
    "        # Rank best prediction, then Save\n",
    "        final_results[lang_out].append(get_best_prediction(row_results, is_stat=(True if \"stat\" in model else False)))\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "074684b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_models = ['shared_bilingual', 'shared_multilingual', 'shared_statistical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8ef3f45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK abrahammonpa 0.10 shared_bilingual\n",
      "OK abrahammonpa 0.10 shared_multilingual\n",
      "OK abrahammonpa 0.10 shared_statistical\n",
      "OK abrahammonpa 0.20 shared_bilingual\n",
      "OK abrahammonpa 0.20 shared_multilingual\n",
      "OK abrahammonpa 0.20 shared_statistical\n",
      "OK abrahammonpa 0.30 shared_bilingual\n",
      "OK abrahammonpa 0.30 shared_multilingual\n",
      "OK abrahammonpa 0.30 shared_statistical\n",
      "OK abrahammonpa 0.40 shared_bilingual\n",
      "OK abrahammonpa 0.40 shared_multilingual\n",
      "OK abrahammonpa 0.40 shared_statistical\n",
      "OK abrahammonpa 0.50 shared_bilingual\n",
      "OK abrahammonpa 0.50 shared_multilingual\n",
      "OK abrahammonpa 0.50 shared_statistical\n",
      "OK allenbai 0.10 shared_bilingual\n",
      "OK allenbai 0.10 shared_multilingual\n",
      "OK allenbai 0.10 shared_statistical\n",
      "OK allenbai 0.20 shared_bilingual\n",
      "OK allenbai 0.20 shared_multilingual\n",
      "OK allenbai 0.20 shared_statistical\n",
      "OK allenbai 0.30 shared_bilingual\n",
      "OK allenbai 0.30 shared_multilingual\n",
      "OK allenbai 0.30 shared_statistical\n",
      "OK allenbai 0.40 shared_bilingual\n",
      "OK allenbai 0.40 shared_multilingual\n",
      "OK allenbai 0.40 shared_statistical\n",
      "OK allenbai 0.50 shared_bilingual\n",
      "OK allenbai 0.50 shared_multilingual\n",
      "OK allenbai 0.50 shared_statistical\n",
      "OK backstromnorthernpakistan 0.10 shared_bilingual\n",
      "OK backstromnorthernpakistan 0.10 shared_multilingual\n",
      "OK backstromnorthernpakistan 0.10 shared_statistical\n",
      "OK backstromnorthernpakistan 0.20 shared_bilingual\n",
      "OK backstromnorthernpakistan 0.20 shared_multilingual\n",
      "OK backstromnorthernpakistan 0.20 shared_statistical\n",
      "OK backstromnorthernpakistan 0.30 shared_bilingual\n",
      "OK backstromnorthernpakistan 0.30 shared_multilingual\n",
      "OK backstromnorthernpakistan 0.30 shared_statistical\n",
      "OK backstromnorthernpakistan 0.40 shared_bilingual\n",
      "OK backstromnorthernpakistan 0.40 shared_multilingual\n",
      "OK backstromnorthernpakistan 0.40 shared_statistical\n",
      "OK backstromnorthernpakistan 0.50 shared_bilingual\n",
      "OK backstromnorthernpakistan 0.50 shared_multilingual\n",
      "OK backstromnorthernpakistan 0.50 shared_statistical\n",
      "OK castrosui 0.10 shared_bilingual\n",
      "OK castrosui 0.10 shared_multilingual\n",
      "OK castrosui 0.10 shared_statistical\n",
      "OK castrosui 0.20 shared_bilingual\n",
      "OK castrosui 0.20 shared_multilingual\n",
      "OK castrosui 0.20 shared_statistical\n",
      "OK castrosui 0.30 shared_bilingual\n",
      "OK castrosui 0.30 shared_multilingual\n",
      "OK castrosui 0.30 shared_statistical\n",
      "OK castrosui 0.40 shared_bilingual\n",
      "OK castrosui 0.40 shared_multilingual\n",
      "OK castrosui 0.40 shared_statistical\n",
      "Problem! Several plausible keys! pʰ uː t ¹ ['pʰ <unk> t ¹', 'p <unk> t ¹']\n",
      "Plausible keys chosen is  pʰ <unk> t ¹\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'BanliangYangAn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1967913/580915992.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;31m# Reorder according to initial file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mfinal_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreordering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_by_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;31m# Store best prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1967913/989399453.py\u001b[0m in \u001b[0;36mreordering\u001b[0;34m(raw_data_path, cur_data, model, split, results_by_lang)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# We filter on length ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mrow_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;36m0.3\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'BanliangYangAn'"
     ]
    }
   ],
   "source": [
    "for cur_data in folders:\n",
    "    for split in splits:\n",
    "        df = pd.read_csv(os.path.join(raw_data_path, cur_data, f\"cognates.tsv\"), sep=\"\\t\")\n",
    "        langs = [c for c in df.columns if c != 'COGID']\n",
    "        results_grouped = {model: {lang: defaultdict(dict) for lang in langs} for model in loc_models}\n",
    "        results_by_lang = {model: {lang: defaultdict(dict) for lang in langs} for model in loc_models}\n",
    "\n",
    "        for model in loc_models:\n",
    "            if True: #try:\n",
    "                # Read results\n",
    "                cur_results_grouped, cur_results_by_lang = get_results_from_file(langs, model, cur_data)\n",
    "                results_grouped[model] = cur_results_grouped\n",
    "                results_by_lang[model] = cur_results_by_lang\n",
    "\n",
    "                # Reorder according to initial file\n",
    "                final_results = reordering(raw_data_path, cur_data, model, split, results_by_lang)\n",
    "\n",
    "                # Store best prediction\n",
    "                if False:\n",
    "                    with open(os.path.join(raw_data_path, cur_data, f\"results-{model}-{split}.tsv\"), \"w+\") as f:\n",
    "                        f.write(\"COGID\\t\" + \"\\t\".join(langs) + \"\\n\")\n",
    "                        for ix in range(len(final_results[\"COGID\"])):\n",
    "                            try:\n",
    "                                f.write(\"\\t\".join([final_results[label][ix] if final_results[label][ix] else \"\" for label in [\"COGID\"] + langs]) + \"\\n\")\n",
    "                            except IndexError as e:\n",
    "                                raise e\n",
    "                print(\"OK\", cur_data, split, model)\n",
    "            #except Exception as e:\n",
    "            #    print(\"ERROR\", cur_data, split, model, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab02ee16",
   "metadata": {},
   "source": [
    "```bash\n",
    "source ~/Desktop/SIGTYP2022/venv/bin/activate\n",
    "# Baseline\n",
    "#model=\"backstromnorthernpakistan\"\n",
    "for model in \"abrahammonpa\" \"allenbai\" \"backstromnorthernpakistan\" \"castrosui\" \"davletshinaztecan\" \"felekesemitic\" \"hantganbangime\" \"hattorijaponic\" \"listsamplesize\" \"mannburmish\"; do\n",
    "    echo \")0.10 - baseline\" >> analysis_${model}.txt;\n",
    "    st2022 --compare --prediction-file=${model}/result-0.10.tsv --solution-file=${model}/solutions-0.10.tsv >> analysis_${model}.txt;\n",
    "    echo \")0.10 - BiNMT\" >> analysis_${model}.txt;\n",
    "    st2022 --compare --prediction-file=${model}/results-shared_bilingual-0.10.tsv --solution-file=${model}/solutions-0.10.tsv  >> analysis_${model}.txt;\n",
    "    echo \")0.10 - MNMT\" >> analysis_${model}.txt;\n",
    "    st2022 --compare --prediction-file=${model}/results-shared_multilingual-0.10.tsv --solution-file=${model}/solutions-0.10.tsv  >> analysis_${model}.txt;\n",
    "    echo \")0.10 - SMT\" >> analysis_${model}.txt;\n",
    "    st2022 --compare --prediction-file=${model}/results-shared_statistical-0.10.tsv --solution-file=${model}/solutions-0.10.tsv  >> analysis_${model}.txt;\n",
    "    \n",
    "    echo \")0.50 - baseline\" >> analysis_${model}.txt;\n",
    "    st2022 --compare --prediction-file=${model}/result-0.50.tsv --solution-file=${model}/solutions-0.50.tsv  >> analysis_${model}.txt;\n",
    "    echo \")0.50 - BiNMT\" >> analysis_${model}.txt;\n",
    "    st2022 --compare --prediction-file=${model}/results-shared_bilingual-0.50.tsv --solution-file=${model}/solutions-0.50.tsv >> analysis_${model}.txt;\n",
    "    echo \")0.50 - MNMT\" >> analysis_${model}.txt;\n",
    "    st2022 --compare --prediction-file=${model}/results-shared_multilingual-0.50.tsv --solution-file=${model}/solutions-0.50.tsv  >> analysis_${model}.txt;\n",
    "    echo \")0.50 - SMT\" >> analysis_${model}.txt;\n",
    "    st2022 --compare --prediction-file=${model}/results-shared_statistical-0.50.tsv --solution-file=${model}/solutions-0.50.tsv >> analysis_${model}.txt;\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fe811",
   "metadata": {},
   "source": [
    "## Problems to manage\n",
    "- sur allenbai encoding en 0.10 pour bilingue et multilingue\n",
    "- castrosui languages changes for saves, need to change them when called here too\n",
    "- davletshinaztecan 0.20 bilingual not launched for all language pairs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a02ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {language_family: {split: {\n",
    "    model: {} for model in models\n",
    "} for split in splits} for language_family in folders}\n",
    "\n",
    "for language_family in folders:\n",
    "    cur_model = \"baseline\"\n",
    "    cur_split = \"0.10\"\n",
    "    with open(os.path.join(copper_dir, raw_data_path, f\"analysis_{language_family}.txt\"), \"r\") as f: \n",
    "        for line in f:\n",
    "            if \"Language\" in line: continue\n",
    "            if \"--\" in line: continue\n",
    "            if line[0] == \")\":\n",
    "                cur_split, cur_model = line[1:].replace(\"\\n\", \"\").split(\" - \");\n",
    "                continue\n",
    "            lang, ed, norm_ed, f5 = \" \".join(line.split()).split()\n",
    "            results_dict[language_family][cur_split][cur_model].update(\n",
    "                {lang: {\"ED\": ed, \"Normalized ED\": norm_ed, \"B2 F5\": f5}}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0fcf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in results_dict.items():\n",
    "    for split in [\"0.10\", \"0.50\"]:\n",
    "        for model in models:\n",
    "            try:\n",
    "                print(k, split, v[split][model][\"TOTAL\"])\n",
    "            except KeyError:\n",
    "                print(k, model, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a7524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in results_dict.items():\n",
    "    for split in [\"0.10\", \"0.50\"]:\n",
    "        for model in models:\n",
    "            try:\n",
    "                print(k, split, v[split][model][\"TOTAL\"])\n",
    "            except KeyError:\n",
    "                print(k, model, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1accadaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
