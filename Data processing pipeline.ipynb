{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c005eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "840378de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26cedf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding module to sys path\n",
    "import sys\n",
    "sys.path.append(\"/home/cfourrie/documents/software/public/CopperMT/\")\n",
    "# RNN imports\n",
    "import pipeline\n",
    "import torch, numpy as np\n",
    "from fairseq import checkpoint_utils, data, options, tasks\n",
    "from pipeline.neural_translation.multilingual_rnns.multilingual_rnn import MultilingualRNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b414a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "copper_dir = \"/home/cfourrie/documents/software/public/CopperMT/\"\n",
    "raw_data_path = \"inputs/raw_data/\" \n",
    "split_data_path = \"inputs/split_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b84c2256",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"abrahammonpa\", \"allenbai\", \"backstromnorthernpakistan\", \"castrosui\", \"davletshinaztecan\", \n",
    "           \"felekesemitic\", \"hantganbangime\", \"hattorijaponic\", \"listsamplesize\", \"mannburmish\"]\n",
    "splits = [\"0.10\", \"0.20\", \"0.30\", \"0.40\", \"0.50\"]\n",
    "models = [\"baseline\", \"BiNMT\", \"MNMT\", \"SMT\"]\n",
    "train_name = \"training\"\n",
    "test_in_name = \"test\"\n",
    "test_out_name = \"solutions\"\n",
    "save_as = \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ddbf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_needed = False\n",
    "preprocessing_needed = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af2e2cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"/home/cfourrie/documents/software/public/CopperMT/workspace\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26858afc",
   "metadata": {},
   "source": [
    "## Convert data to usual format for our software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ef4e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you do not want to share embeddings, removed shared_ from the folder path\n",
    "if conversion_needed:\n",
    "    for folder in folders:\n",
    "        for split in splits:\n",
    "            try: os.makedirs(os.path.join(split_data_path, f\"shared_{folder}\", split))\n",
    "            except FileExistsError: pass\n",
    "            # Training\n",
    "            cur_data = pd.read_csv(os.path.join(raw_data_path, folder, f\"training-{split}.tsv\"), sep=\"\\t\")\n",
    "            cur_languages = [c for c in cur_data.columns if c != 'COGID']\n",
    "            if folder == \"castrosui\":\n",
    "                cur_languages = [re.split('(?=[A-Z])', l)[1] for l in cur_languages]\n",
    "                cur_data.columns = ['COGID'] + cur_languages\n",
    "            for l1, l2 in itertools.combinations(cur_languages, 2):\n",
    "                for name in [f\"{l1}-{l2}\", f\"{l2}-{l1}\"]:\n",
    "                    with open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"train.{name}.{l1}\"), \"w+\") as f1, \\\n",
    "                         open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"train.{name}.{l2}\"), \"w+\") as f2:\n",
    "                        for ix, row in cur_data[[l1, l2]].dropna().iterrows():\n",
    "                            f1.write(row[l1] + \"\\n\")\n",
    "                            f2.write(row[l2] + \"\\n\")\n",
    "            for l in cur_languages:\n",
    "                with open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"train.{l}-{l}.{l}\"), \"w+\") as f:\n",
    "                    for ix, row in cur_data[[l]].dropna().iterrows():\n",
    "                        f.write(row[l] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6855a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of test and validation sets \n",
    "# If you do not want to share embeddings, removed shared_ from the folder path\n",
    "if conversion_needed:\n",
    "    for folder in folders:\n",
    "        for split in splits:\n",
    "            try: os.makedirs(os.path.join(split_data_path, f\"shared_{folder}\", split))\n",
    "            except FileExistsError: pass\n",
    "            # Training\n",
    "            cur_data = pd.read_csv(os.path.join(raw_data_path, folder, f\"test-{split}.tsv\"), sep=\"\\t\")\n",
    "            cur_languages = [c for c in cur_data.columns if c != 'COGID']\n",
    "            if folder == \"castrosui\":\n",
    "                cur_languages = [re.split('(?=[A-Z])', l)[1] for l in cur_languages]\n",
    "                cur_data.columns = ['COGID'] + cur_languages\n",
    "\n",
    "            for l1, l2 in itertools.combinations(cur_languages, 2):\n",
    "                with open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"test.{l1}-{l2}.{l1}\"), \"w+\") as f1_test, \\\n",
    "                     open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"test.{l2}-{l1}.{l2}\"), \"w+\") as f2_test, \\\n",
    "                     open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"valid.{l1}-{l2}.{l1}\"), \"w+\") as f1_val, \\\n",
    "                     open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"valid.{l1}-{l2}.{l2}\"), \"w+\") as f2_val, \\\n",
    "                     open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"valid.{l2}-{l1}.{l1}\"), \"w+\") as f1_val_r, \\\n",
    "                     open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"valid.{l2}-{l1}.{l2}\"), \"w+\") as f2_val_r:\n",
    "                        for ix, row in cur_data[[l1, l2]].dropna().iterrows():\n",
    "                            if row[l1] != \"?\" and row[l2] != \"?\":                                \n",
    "                                f1_val.write(row[l1] + \"\\n\")                            \n",
    "                                f1_val_r.write(row[l1] + \"\\n\")                            \n",
    "                                f2_val.write(row[l2] + \"\\n\")                            \n",
    "                                f2_val_r.write(row[l2] + \"\\n\")\n",
    "                            if row[l1] == \"?\" and row[l2] != \"?\":\n",
    "                                f2_test.write(row[l2] + \"\\n\")\n",
    "                            if row[l2] == \"?\" and row[l1] != \"?\":\n",
    "                                f1_test.write(row[l1] + \"\\n\")\n",
    "\n",
    "            for l in cur_languages:\n",
    "                with open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"valid.{l}-{l}.{l}\"), \"w+\") as f:\n",
    "                    for ix, row in cur_data[[l]].dropna().iterrows():\n",
    "                        if row[l] != \"?\":\n",
    "                            f.write(row[l] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e05e28",
   "metadata": {},
   "source": [
    "## Generating configuration files contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d7b2426",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if preprocessing_needed:\n",
    "    folder = folders[3]\n",
    "    cur_data = pd.read_csv(os.path.join(raw_data_path, folder, f\"cognates.tsv\"), sep=\"\\t\")\n",
    "    cur_languages = [c for c in cur_data.columns if c != 'COGID']\n",
    "    if folder == \"castrosui\":\n",
    "        cur_languages = [re.split('(?=[A-Z])', l)[1] for l in cur_languages]\n",
    "\n",
    "    print('PROJ_DIR=\"/home/cfourrie/documents/software/CopperMT\"')\n",
    "    print('MOSES_DIR=\"${PROJ_DIR}/submodules\"')\n",
    "    print()\n",
    "    print('WK_DIR=\"${PROJ_DIR}/workspace/' + folder + '\"')\n",
    "    print('INPUTS_DIR=\"${PROJ_DIR}/inputs\"')\n",
    "    print()\n",
    "    print(f'DATA_NAME=\"{folder}\"')\n",
    "\n",
    "    print(f'langs_bi=\"{\",\".join(\"-\".join(l) for l in itertools.product(cur_languages, cur_languages))}\"')\n",
    "    print(f'langs=\"{\",\".join(cur_languages)}\"')\n",
    "    print(f'langs_shared=\"{\"-\".join(cur_languages)}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf8738",
   "metadata": {},
   "source": [
    "Step: 1h30 - In the pipeline folder, execute:\n",
    "\n",
    "`\n",
    "bash data_preprocess.sh parameters_abrahammonpa.cfg\n",
    "bash data_preprocess.sh parameters_allenbai.cfg\n",
    "bash data_preprocess.sh parameters_backstromnorthernpakistan.cfg\n",
    "bash data_preprocess.sh parameters_castrosui.cfg\n",
    "bash data_preprocess.sh parameters_davletshinaztecan.cfg\n",
    "bash data_preprocess.sh parameters_felekesemitic.cfg\n",
    "bash data_preprocess.sh parameters_hantganbangime.cfg\n",
    "bash data_preprocess.sh parameters_hattorijaponic.cfg\n",
    "bash data_preprocess.sh parameters_listsamplesize.cfg\n",
    "bash data_preprocess.sh parameters_mannburmish.cfg\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a787af",
   "metadata": {},
   "source": [
    "## Training models\n",
    "\n",
    "In the pipeline folder, train bilingual neural models with:\n",
    "\n",
    "`\n",
    "bash main_nmt_bilingual_full.sh parameters_abrahammonpa.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_allenbai.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_backstromnorthernpakistan.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_castrosui.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_davletshinaztecan.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_felekesemitic.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_hantganbangime.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_hattorijaponic.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_listsamplesize.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_mannburmish.cfg\n",
    "`\n",
    "\n",
    "In the pipeline folder, train multilingual neural models with:\n",
    "\n",
    "`\n",
    "bash main_nmt_multilingual_full.sh parameters_abrahammonpa.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_allenbai.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_backstromnorthernpakistan.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_castrosui.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_davletshinaztecan.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_felekesemitic.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_hantganbangime.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_hattorijaponic.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_listsamplesize.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_mannburmish.cfg\n",
    "`\n",
    "\n",
    "\n",
    "In the pipeline folder, train bilingual statistical models with:\n",
    "\n",
    "`\n",
    "bash main_smt_full.sh parameters_abrahammonpa.cfg\n",
    "bash main_smt_full.sh parameters_allenbai.cfg\n",
    "bash main_smt_full.sh parameters_backstromnorthernpakistan.cfg\n",
    "bash main_smt_full.sh parameters_castrosui.cfg\n",
    "bash main_smt_full.sh parameters_davletshinaztecan.cfg\n",
    "bash main_smt_full.sh parameters_felekesemitic.cfg\n",
    "bash main_smt_full.sh parameters_hantganbangime.cfg\n",
    "bash main_smt_full.sh parameters_hattorijaponic.cfg\n",
    "bash main_smt_full.sh parameters_listsamplesize.cfg\n",
    "bash main_smt_full.sh parameters_mannburmish.cfg\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4299500",
   "metadata": {},
   "source": [
    "# Choosing best answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b2ed5",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b163463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sublist_with_unk(ls_with_unk, ls):\n",
    "    ls = \"\".join(ls)\n",
    "    ls_with_unk = \"\".join(ls_with_unk)\n",
    "    \n",
    "    for item in ls_with_unk.split(\"<unk>\"):\n",
    "        if item not in ls:\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "260ea9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neural_bleu_predictions(path, l_in, l_out, n_best):\n",
    "    # Storage\n",
    "    source = []\n",
    "    target = []\n",
    "    prediction = []\n",
    "    confidence = []\n",
    "    cur_prediction = []\n",
    "    cur_confidence = []\n",
    "    indices = []\n",
    "    with open(\n",
    "            f'{path}/bleu/bleu_checkpoint_best_{l_in}-{l_out}.{l_out}', 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            line = line.split(\"\\t\")\n",
    "            # Actual source\n",
    "            if \"S-\" in line[0]:\n",
    "                word = line[1].strip(' ').split()\n",
    "                source.append(word)\n",
    "                indices.append(line[0])\n",
    "                # We reinitialize the cur_prediction list\n",
    "                if len(cur_prediction) > 0:\n",
    "                    prediction.append(cur_prediction)\n",
    "                    confidence.append(cur_confidence)\n",
    "                    cur_prediction = []\n",
    "                    cur_confidence = []\n",
    "            # Actual target\n",
    "            if \"T-\" in line[0]:\n",
    "                word = line[1].strip(' ').split()\n",
    "                target.append(word)\n",
    "            # Hypothesis\n",
    "            if \"H-\" in line[0] and len(cur_prediction) < n_best:\n",
    "                word = line[2].strip(' ').split()\n",
    "                cur_prediction.append(word)\n",
    "                cur_confidence.append(math.exp(float(line[1])))\n",
    "        prediction.append(cur_prediction)\n",
    "        confidence.append(cur_confidence)\n",
    "        try:\n",
    "            prediction = [[bor[n] for bor in prediction] for n in range(n_best)]\n",
    "        except IndexError as e:\n",
    "            raise e\n",
    "\n",
    "        #prediction = [[bor[n] for bor in prediction] for n in range(n_best)]\n",
    "\n",
    "    return source, target, prediction, confidence, indices\n",
    "\n",
    "\n",
    "def get_statistical_bleu_predictions(path_data, path, l_in, l_out, n_best, cur_n_best):\n",
    "    target = []\n",
    "    try:\n",
    "        with open(f'{path_data}/test.{l_in}-{l_out}.{l_out}', 'r') as file:\n",
    "            for i, line in enumerate(file):\n",
    "                target.append(line.split())\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    source = []\n",
    "    with open(f'{path_data}/test.{l_in}-{l_out}.{l_in}', 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            source.append(line.split())\n",
    "\n",
    "    prediction = []\n",
    "    confidence = []\n",
    "    cur_ix = -1\n",
    "    cur_prediction = []\n",
    "    cur_confidence = []\n",
    "    indices = []\n",
    "    with open(f'{path}/{l_in}-{l_out}/out/'\n",
    "              f'test.{l_in}-{l_out}_nbest_{str(n_best)}.{l_out}', 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            line = line.split(\"|||\")\n",
    "            ix = int(line[0])\n",
    "            word = line[1].strip(' ').split()\n",
    "\n",
    "            if cur_ix != ix:\n",
    "                if cur_ix != -1:\n",
    "                    indices.append(cur_ix)\n",
    "                    while len(cur_prediction) < cur_n_best:\n",
    "                        cur_prediction.append(cur_prediction[-1])\n",
    "                        cur_confidence.append(cur_confidence[-1])\n",
    "                    prediction.append(cur_prediction)\n",
    "                    confidence.append(cur_confidence)\n",
    "                cur_prediction = [word]\n",
    "                cur_confidence = [math.exp(float(line[-1]))]\n",
    "                cur_ix = ix\n",
    "            else:\n",
    "                cur_prediction.append(word)\n",
    "                cur_confidence.append(math.exp(float(line[-1])))\n",
    "        # Management of last prediction\n",
    "        indices.append(cur_ix)\n",
    "        while len(cur_prediction) < cur_n_best:\n",
    "            cur_prediction.append(cur_prediction[-1])\n",
    "            cur_confidence.append(cur_confidence[-1])\n",
    "        prediction.append(cur_prediction)\n",
    "        confidence.append(cur_confidence)\n",
    "\n",
    "    prediction = [[bor[n] for bor in prediction] for n in range(cur_n_best)]\n",
    "\n",
    "    return source, target, prediction, confidence, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226caa2c",
   "metadata": {},
   "source": [
    "## Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b1fa3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_file(langs, model, cur_data):\n",
    "    cur_results_grouped = {lang: defaultdict(dict) for lang in langs}\n",
    "    cur_results_by_lang = {lang: defaultdict(dict) for lang in langs}\n",
    "        # Read results\n",
    "    for lang_out in langs:\n",
    "        for lang_in in langs:\n",
    "            if lang_in == lang_out: continue\n",
    "            if model == \"shared_bilingual\":\n",
    "                all_sources, _, all_predictions, all_confidences, all_indices = get_neural_bleu_predictions(\n",
    "                    f\"{models_dir}/{cur_data}/{model}/{lang_in}-{lang_out}/{split}\", lang_in, lang_out, 10)\n",
    "            elif model == \"shared_multilingual\":\n",
    "                all_sources, _, all_predictions, all_confidences, all_indices = get_neural_bleu_predictions(\n",
    "                    f\"{models_dir}/{cur_data}/{model}/{split}\", lang_in, lang_out, 10)\n",
    "            else:\n",
    "                all_sources, _, all_predictions, all_confidences, all_indices = get_statistical_bleu_predictions(\n",
    "                    f\"{split_data_path}/shared_{cur_data}/{split}\",\n",
    "                    f\"{models_dir}/{cur_data}/{model}/{split}\", lang_in, lang_out, 10, 10)\n",
    "             \n",
    "            for ix, (source, index) in enumerate(zip(all_sources, all_indices)):\n",
    "                predictions = [\" \".join(all_predictions[n_best][ix]) for n_best in range(10)]\n",
    "                confidences = all_confidences[ix]\n",
    "                cur_results_grouped[lang_out][index].update(\n",
    "                    {f\"{lang_in}_source\": \" \".join(source),\n",
    "                     lang_in: sorted([(p, c) for p, c in zip(predictions, confidences)])}\n",
    "                ) \n",
    "                cur_results_by_lang[lang_out][lang_in].update(\n",
    "                     {\" \".join(source): sorted([(p, c) for p, c in zip(predictions, confidences)])}\n",
    "                ) \n",
    "                \n",
    "                \n",
    "    return cur_results_grouped, cur_results_by_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8b7a719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_prediction(row_results):\n",
    "    predictions_scores = defaultdict(int)\n",
    "    predictions_counts = defaultdict(int)\n",
    "    for lang_res in row_results.values():\n",
    "        for pred, score in lang_res:\n",
    "            predictions_scores[pred] += score\n",
    "            predictions_counts[pred] += 1\n",
    "    # prediction scores is better for SMT models! (considerably)\n",
    "    best_prediction = [k for k, v in predictions_scores.items() if v == max(predictions_scores.values())]\n",
    "    if best_prediction:\n",
    "        best_prediction = best_prediction[0]\n",
    "    else:\n",
    "        best_prediction = \"\"\n",
    "\n",
    "    return best_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "529ae67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reordering(raw_data_path, cur_data, model, split, results_by_lang):\n",
    "    test_df = pd.read_csv(os.path.join(raw_data_path, cur_data, f\"test-{split}.tsv\"), sep=\"\\t\")\n",
    "    final_results = defaultdict(list)\n",
    "    for ix, (_, row) in enumerate(test_df.iterrows()):\n",
    "        row_dict = dict(row)\n",
    "        final_results[\"COGID\"].append(row_dict.pop(\"COGID\"))\n",
    "        lang_out = [k for k, v in row_dict.items() if v == \"?\"][0]\n",
    "        row_results = {}\n",
    "        # Compute all predictions\n",
    "        for lang, val in row_dict.items():\n",
    "            if lang == lang_out: continue\n",
    "                \n",
    "            final_results[lang].append(\"\")                \n",
    "            if not isinstance(val, str): continue # nan because was empty\n",
    "            try:\n",
    "                row_results[lang] = results_by_lang[model][lang_out][lang][val]\n",
    "            except KeyError: # some chars are only present in test, and encoded as unk\n",
    "                # We extract possible keys \n",
    "                keys_with_unk = [v for v in results_by_lang[model][lang_out][lang].keys() \n",
    "                                 if \"<unk>\" in v and len(v.split(\" \")) == len(val.split(\" \"))]\n",
    "                possible_keys = []\n",
    "                for key in keys_with_unk:\n",
    "                    if sublist_with_unk(key.split(\" \"), val.split(\" \")):\n",
    "                        possible_keys.append(key)\n",
    "\n",
    "                if len(possible_keys) > 1:\n",
    "                    raise Exception(\"Problem! Several plausible keys!\", val, possible_keys)\n",
    "                elif len(possible_keys) == 0:\n",
    "                    raise Exception(\"Problem! No plausible key!\", val, lang, lang_out, results_by_lang[model][lang_out])\n",
    "                else:\n",
    "                    row_results[lang] = results_by_lang[model][lang_out][lang][possible_keys[0]]\n",
    "                    \n",
    "            # We filter on length ratio\n",
    "            row_results[lang] = [v for v in row_results[lang] if 0.3 < len(v)/len(val) < 3]\n",
    "\n",
    "\n",
    "        # Rank best prediction, then Save\n",
    "        final_results[lang_out].append(get_best_prediction(row_results))\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "074684b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_models = ['shared_bilingual', 'shared_multilingual', 'shared_statistical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e8ef3f45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK abrahammonpa 0.10 shared_bilingual\n",
      "OK abrahammonpa 0.10 shared_multilingual\n",
      "OK abrahammonpa 0.10 shared_statistical\n",
      "OK abrahammonpa 0.20 shared_bilingual\n",
      "OK abrahammonpa 0.20 shared_multilingual\n",
      "OK abrahammonpa 0.20 shared_statistical\n",
      "OK abrahammonpa 0.30 shared_bilingual\n",
      "OK abrahammonpa 0.30 shared_multilingual\n",
      "OK abrahammonpa 0.30 shared_statistical\n",
      "OK abrahammonpa 0.40 shared_bilingual\n",
      "OK abrahammonpa 0.40 shared_multilingual\n",
      "OK abrahammonpa 0.40 shared_statistical\n",
      "OK abrahammonpa 0.50 shared_bilingual\n",
      "OK abrahammonpa 0.50 shared_multilingual\n",
      "OK abrahammonpa 0.50 shared_statistical\n",
      "ERROR allenbai 0.10 shared_bilingual list index out of range\n",
      "ERROR allenbai 0.10 shared_multilingual list index out of range\n",
      "ERROR allenbai 0.10 shared_statistical list index out of range\n",
      "OK allenbai 0.20 shared_bilingual\n",
      "OK allenbai 0.20 shared_multilingual\n",
      "OK allenbai 0.20 shared_statistical\n",
      "OK allenbai 0.30 shared_bilingual\n",
      "OK allenbai 0.30 shared_multilingual\n",
      "OK allenbai 0.30 shared_statistical\n",
      "OK allenbai 0.40 shared_bilingual\n",
      "OK allenbai 0.40 shared_multilingual\n",
      "OK allenbai 0.40 shared_statistical\n",
      "OK allenbai 0.50 shared_bilingual\n",
      "OK allenbai 0.50 shared_multilingual\n",
      "OK allenbai 0.50 shared_statistical\n",
      "OK backstromnorthernpakistan 0.10 shared_bilingual\n",
      "OK backstromnorthernpakistan 0.10 shared_multilingual\n",
      "OK backstromnorthernpakistan 0.10 shared_statistical\n",
      "OK backstromnorthernpakistan 0.20 shared_bilingual\n",
      "OK backstromnorthernpakistan 0.20 shared_multilingual\n",
      "OK backstromnorthernpakistan 0.20 shared_statistical\n",
      "OK backstromnorthernpakistan 0.30 shared_bilingual\n",
      "OK backstromnorthernpakistan 0.30 shared_multilingual\n",
      "OK backstromnorthernpakistan 0.30 shared_statistical\n",
      "OK backstromnorthernpakistan 0.40 shared_bilingual\n",
      "OK backstromnorthernpakistan 0.40 shared_multilingual\n",
      "OK backstromnorthernpakistan 0.40 shared_statistical\n",
      "OK backstromnorthernpakistan 0.50 shared_bilingual\n",
      "OK backstromnorthernpakistan 0.50 shared_multilingual\n",
      "OK backstromnorthernpakistan 0.50 shared_statistical\n",
      "ERROR castrosui 0.10 shared_bilingual [Errno 2] No such file or directory: '/home/cfourrie/documents/software/public/CopperMT/workspace/castrosui/shared_bilingual/BanliangYangAn-AntangWesternSandong/0.10/bleu/bleu_checkpoint_best_BanliangYangAn-AntangWesternSandong.AntangWesternSandong'\n",
      "ERROR castrosui 0.10 shared_multilingual [Errno 2] No such file or directory: '/home/cfourrie/documents/software/public/CopperMT/workspace/castrosui/shared_multilingual/0.10/bleu/bleu_checkpoint_best_BanliangYangAn-AntangWesternSandong.AntangWesternSandong'\n",
      "ERROR castrosui 0.10 shared_statistical [Errno 2] No such file or directory: 'inputs/split_data//shared_castrosui/0.10/test.BanliangYangAn-AntangWesternSandong.BanliangYangAn'\n",
      "ERROR castrosui 0.20 shared_bilingual [Errno 2] No such file or directory: '/home/cfourrie/documents/software/public/CopperMT/workspace/castrosui/shared_bilingual/BanliangYangAn-AntangWesternSandong/0.20/bleu/bleu_checkpoint_best_BanliangYangAn-AntangWesternSandong.AntangWesternSandong'\n",
      "ERROR castrosui 0.20 shared_multilingual [Errno 2] No such file or directory: '/home/cfourrie/documents/software/public/CopperMT/workspace/castrosui/shared_multilingual/0.20/bleu/bleu_checkpoint_best_BanliangYangAn-AntangWesternSandong.AntangWesternSandong'\n",
      "ERROR castrosui 0.20 shared_statistical [Errno 2] No such file or directory: 'inputs/split_data//shared_castrosui/0.20/test.BanliangYangAn-AntangWesternSandong.BanliangYangAn'\n",
      "ERROR castrosui 0.30 shared_bilingual [Errno 2] No such file or directory: '/home/cfourrie/documents/software/public/CopperMT/workspace/castrosui/shared_bilingual/BanliangYangAn-AntangWesternSandong/0.30/bleu/bleu_checkpoint_best_BanliangYangAn-AntangWesternSandong.AntangWesternSandong'\n",
      "ERROR castrosui 0.30 shared_multilingual [Errno 2] No such file or directory: '/home/cfourrie/documents/software/public/CopperMT/workspace/castrosui/shared_multilingual/0.30/bleu/bleu_checkpoint_best_BanliangYangAn-AntangWesternSandong.AntangWesternSandong'\n",
      "ERROR castrosui 0.30 shared_statistical [Errno 2] No such file or directory: 'inputs/split_data//shared_castrosui/0.30/test.BanliangYangAn-AntangWesternSandong.BanliangYangAn'\n",
      "ERROR castrosui 0.40 shared_bilingual [Errno 2] No such file or directory: '/home/cfourrie/documents/software/public/CopperMT/workspace/castrosui/shared_bilingual/BanliangYangAn-AntangWesternSandong/0.40/bleu/bleu_checkpoint_best_BanliangYangAn-AntangWesternSandong.AntangWesternSandong'\n",
      "ERROR castrosui 0.40 shared_multilingual [Errno 2] No such file or directory: '/home/cfourrie/documents/software/public/CopperMT/workspace/castrosui/shared_multilingual/0.40/bleu/bleu_checkpoint_best_BanliangYangAn-AntangWesternSandong.AntangWesternSandong'\n",
      "ERROR castrosui 0.40 shared_statistical [Errno 2] No such file or directory: 'inputs/split_data//shared_castrosui/0.40/test.BanliangYangAn-AntangWesternSandong.BanliangYangAn'\n",
      "ERROR castrosui 0.50 shared_bilingual [Errno 2] No such file or directory: '/home/cfourrie/documents/software/public/CopperMT/workspace/castrosui/shared_bilingual/BanliangYangAn-AntangWesternSandong/0.50/bleu/bleu_checkpoint_best_BanliangYangAn-AntangWesternSandong.AntangWesternSandong'\n",
      "ERROR castrosui 0.50 shared_multilingual [Errno 2] No such file or directory: '/home/cfourrie/documents/software/public/CopperMT/workspace/castrosui/shared_multilingual/0.50/bleu/bleu_checkpoint_best_BanliangYangAn-AntangWesternSandong.AntangWesternSandong'\n",
      "ERROR castrosui 0.50 shared_statistical [Errno 2] No such file or directory: 'inputs/split_data//shared_castrosui/0.50/test.BanliangYangAn-AntangWesternSandong.BanliangYangAn'\n",
      "OK davletshinaztecan 0.10 shared_bilingual\n",
      "OK davletshinaztecan 0.10 shared_multilingual\n",
      "OK davletshinaztecan 0.10 shared_statistical\n",
      "ERROR davletshinaztecan 0.20 shared_bilingual list index out of range\n",
      "OK davletshinaztecan 0.20 shared_multilingual\n",
      "OK davletshinaztecan 0.20 shared_statistical\n",
      "OK davletshinaztecan 0.30 shared_bilingual\n",
      "OK davletshinaztecan 0.30 shared_multilingual\n",
      "OK davletshinaztecan 0.30 shared_statistical\n",
      "OK davletshinaztecan 0.40 shared_bilingual\n",
      "OK davletshinaztecan 0.40 shared_multilingual\n",
      "OK davletshinaztecan 0.40 shared_statistical\n",
      "OK davletshinaztecan 0.50 shared_bilingual\n",
      "OK davletshinaztecan 0.50 shared_multilingual\n",
      "OK davletshinaztecan 0.50 shared_statistical\n",
      "OK felekesemitic 0.10 shared_bilingual\n",
      "ERROR felekesemitic 0.10 shared_multilingual list index out of range\n",
      "OK felekesemitic 0.10 shared_statistical\n",
      "OK felekesemitic 0.20 shared_bilingual\n",
      "ERROR felekesemitic 0.20 shared_multilingual list index out of range\n",
      "OK felekesemitic 0.20 shared_statistical\n",
      "ERROR felekesemitic 0.30 shared_bilingual list index out of range\n",
      "ERROR felekesemitic 0.30 shared_multilingual list index out of range\n",
      "ERROR felekesemitic 0.30 shared_statistical [Errno 2] No such file or directory: '/home/cfourrie/documents/software/public/CopperMT/workspace/felekesemitic/shared_statistical/0.30/Tigrigna-Gura/out/test.Tigrigna-Gura_nbest_10.Gura'\n",
      "ERROR felekesemitic 0.40 shared_bilingual list index out of range\n",
      "ERROR felekesemitic 0.40 shared_multilingual list index out of range\n",
      "ERROR felekesemitic 0.40 shared_statistical [Errno 2] No such file or directory: '/home/cfourrie/documents/software/public/CopperMT/workspace/felekesemitic/shared_statistical/0.40/Endegagn-Amharic/out/test.Endegagn-Amharic_nbest_10.Amharic'\n",
      "ERROR felekesemitic 0.50 shared_bilingual list index out of range\n",
      "ERROR felekesemitic 0.50 shared_multilingual list index out of range\n",
      "ERROR felekesemitic 0.50 shared_statistical [Errno 2] No such file or directory: '/home/cfourrie/documents/software/public/CopperMT/workspace/felekesemitic/shared_statistical/0.50/Chaha-Amharic/out/test.Chaha-Amharic_nbest_10.Amharic'\n",
      "OK hantganbangime 0.10 shared_bilingual\n",
      "ERROR hantganbangime 0.10 shared_multilingual list index out of range\n",
      "OK hantganbangime 0.10 shared_statistical\n",
      "OK hantganbangime 0.20 shared_bilingual\n",
      "ERROR hantganbangime 0.20 shared_multilingual list index out of range\n",
      "OK hantganbangime 0.20 shared_statistical\n",
      "OK hantganbangime 0.30 shared_bilingual\n",
      "ERROR hantganbangime 0.30 shared_multilingual list index out of range\n",
      "OK hantganbangime 0.30 shared_statistical\n",
      "ERROR hantganbangime 0.40 shared_bilingual list index out of range\n",
      "ERROR hantganbangime 0.40 shared_multilingual list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR hantganbangime 0.40 shared_statistical [Errno 2] No such file or directory: '/home/cfourrie/documents/software/public/CopperMT/workspace/hantganbangime/shared_statistical/0.40/Toro_Tegu-Bunoge/out/test.Toro_Tegu-Bunoge_nbest_10.Bunoge'\n",
      "ERROR hantganbangime 0.50 shared_bilingual list index out of range\n",
      "ERROR hantganbangime 0.50 shared_multilingual list index out of range\n",
      "ERROR hantganbangime 0.50 shared_statistical [Errno 2] No such file or directory: '/home/cfourrie/documents/software/public/CopperMT/workspace/hantganbangime/shared_statistical/0.50/Bunoge-Bankan_Tey/out/test.Bunoge-Bankan_Tey_nbest_10.Bankan_Tey'\n",
      "OK hattorijaponic 0.10 shared_bilingual\n",
      "OK hattorijaponic 0.10 shared_multilingual\n",
      "OK hattorijaponic 0.10 shared_statistical\n",
      "OK hattorijaponic 0.20 shared_bilingual\n",
      "OK hattorijaponic 0.20 shared_multilingual\n",
      "OK hattorijaponic 0.20 shared_statistical\n",
      "OK hattorijaponic 0.30 shared_bilingual\n",
      "OK hattorijaponic 0.30 shared_multilingual\n",
      "OK hattorijaponic 0.30 shared_statistical\n",
      "OK hattorijaponic 0.40 shared_bilingual\n",
      "OK hattorijaponic 0.40 shared_multilingual\n",
      "ERROR hattorijaponic 0.40 shared_statistical [Errno 2] No such file or directory: '/home/cfourrie/documents/software/public/CopperMT/workspace/hattorijaponic/shared_statistical/0.40/Kochi-Amami/out/test.Kochi-Amami_nbest_10.Amami'\n",
      "OK hattorijaponic 0.50 shared_bilingual\n",
      "OK hattorijaponic 0.50 shared_multilingual\n",
      "OK hattorijaponic 0.50 shared_statistical\n",
      "OK listsamplesize 0.10 shared_bilingual\n",
      "OK listsamplesize 0.10 shared_multilingual\n",
      "OK listsamplesize 0.10 shared_statistical\n",
      "OK listsamplesize 0.20 shared_bilingual\n",
      "OK listsamplesize 0.20 shared_multilingual\n",
      "OK listsamplesize 0.20 shared_statistical\n",
      "OK listsamplesize 0.30 shared_bilingual\n",
      "OK listsamplesize 0.30 shared_multilingual\n",
      "OK listsamplesize 0.30 shared_statistical\n",
      "OK listsamplesize 0.40 shared_bilingual\n",
      "OK listsamplesize 0.40 shared_multilingual\n",
      "OK listsamplesize 0.40 shared_statistical\n",
      "OK listsamplesize 0.50 shared_bilingual\n",
      "OK listsamplesize 0.50 shared_multilingual\n",
      "OK listsamplesize 0.50 shared_statistical\n",
      "ERROR mannburmish 0.10 shared_bilingual list index out of range\n",
      "ERROR mannburmish 0.10 shared_multilingual list index out of range\n",
      "OK mannburmish 0.10 shared_statistical\n",
      "ERROR mannburmish 0.20 shared_bilingual list index out of range\n",
      "ERROR mannburmish 0.20 shared_multilingual list index out of range\n",
      "OK mannburmish 0.20 shared_statistical\n",
      "ERROR mannburmish 0.30 shared_bilingual list index out of range\n",
      "ERROR mannburmish 0.30 shared_multilingual list index out of range\n",
      "OK mannburmish 0.30 shared_statistical\n",
      "ERROR mannburmish 0.40 shared_bilingual list index out of range\n",
      "ERROR mannburmish 0.40 shared_multilingual list index out of range\n",
      "OK mannburmish 0.40 shared_statistical\n",
      "ERROR mannburmish 0.50 shared_bilingual list index out of range\n",
      "ERROR mannburmish 0.50 shared_multilingual list index out of range\n",
      "OK mannburmish 0.50 shared_statistical\n"
     ]
    }
   ],
   "source": [
    "for cur_data in folders:\n",
    "    for split in splits:\n",
    "        df = pd.read_csv(os.path.join(raw_data_path, cur_data, f\"cognates.tsv\"), sep=\"\\t\")\n",
    "        langs = [c for c in df.columns if c != 'COGID']\n",
    "\n",
    "        results_grouped = {model: {lang: defaultdict(dict) for lang in langs} for model in models}\n",
    "        results_by_lang = {model: {lang: defaultdict(dict) for lang in langs} for model in models}\n",
    "\n",
    "        for model in loc_models:\n",
    "            try:\n",
    "                # Read results\n",
    "                cur_results_grouped, cur_results_by_lang = get_results_from_file(langs, model, cur_data)\n",
    "                results_grouped[model] = cur_results_grouped\n",
    "                results_by_lang[model] = cur_results_by_lang\n",
    "\n",
    "                # Reorder according to initial file\n",
    "                final_results = reordering(raw_data_path, cur_data, model, split, results_by_lang)\n",
    "\n",
    "                # Store best prediction\n",
    "                if True:\n",
    "                    with open(os.path.join(raw_data_path, cur_data, f\"results-{model}-{split}.tsv\"), \"w+\") as f:\n",
    "                        f.write(\"COGID\\t\" + \"\\t\".join(langs) + \"\\n\")\n",
    "                        for ix in range(len(final_results[\"COGID\"])):\n",
    "                            try:\n",
    "                                f.write(\"\\t\".join([final_results[label][ix] if final_results[label][ix] else \"\" for label in [\"COGID\"] + langs]) + \"\\n\")\n",
    "                            except IndexError as e:\n",
    "                                raise e\n",
    "                print(\"OK\", cur_data, split, model)\n",
    "            except Exception as e:\n",
    "                print(\"ERROR\", cur_data, split, model, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab02ee16",
   "metadata": {},
   "source": [
    "```bash\n",
    "source ~/Desktop/SIGTYP2022/venv/bin/activate\n",
    "# Baseline\n",
    "#model=\"backstromnorthernpakistan\"\n",
    "for model in \"abrahammonpa\" \"allenbai\" \"backstromnorthernpakistan\" \"castrosui\" \"davletshinaztecan\" \"felekesemitic\" \"hantganbangime\" \"hattorijaponic\" \"listsamplesize\" \"mannburmish\"; do\n",
    "    echo \")0.10 - baseline\" >> analysis_${model}.txt;\n",
    "    st2022 --compare --prediction-file=${model}/result-0.10.tsv --solution-file=${model}/solutions-0.10.tsv >> analysis_${model}.txt;\n",
    "    echo \")0.10 - BiNMT\" >> analysis_${model}.txt;\n",
    "    st2022 --compare --prediction-file=${model}/results-shared_bilingual-0.10.tsv --solution-file=${model}/solutions-0.10.tsv  >> analysis_${model}.txt;\n",
    "    echo \")0.10 - MNMT\" >> analysis_${model}.txt;\n",
    "    st2022 --compare --prediction-file=${model}/results-shared_multilingual-0.10.tsv --solution-file=${model}/solutions-0.10.tsv  >> analysis_${model}.txt;\n",
    "    echo \")0.10 - SMT\" >> analysis_${model}.txt;\n",
    "    st2022 --compare --prediction-file=${model}/results-shared_statistical-0.10.tsv --solution-file=${model}/solutions-0.10.tsv  >> analysis_${model}.txt;\n",
    "    \n",
    "    echo \")0.50 - baseline\" >> analysis_${model}.txt;\n",
    "    st2022 --compare --prediction-file=${model}/result-0.50.tsv --solution-file=${model}/solutions-0.50.tsv  >> analysis_${model}.txt;\n",
    "    echo \")0.50 - BiNMT\" >> analysis_${model}.txt;\n",
    "    st2022 --compare --prediction-file=${model}/results-shared_bilingual-0.50.tsv --solution-file=${model}/solutions-0.50.tsv >> analysis_${model}.txt;\n",
    "    echo \")0.50 - MNMT\" >> analysis_${model}.txt;\n",
    "    st2022 --compare --prediction-file=${model}/results-shared_multilingual-0.50.tsv --solution-file=${model}/solutions-0.50.tsv  >> analysis_${model}.txt;\n",
    "    echo \")0.50 - SMT\" >> analysis_${model}.txt;\n",
    "    st2022 --compare --prediction-file=${model}/results-shared_statistical-0.50.tsv --solution-file=${model}/solutions-0.50.tsv >> analysis_${model}.txt;\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fe811",
   "metadata": {},
   "source": [
    "## Problems to manage\n",
    "- sur allenbai encoding en 0.10 pour bilingue et multilingue\n",
    "- castrosui languages changes for saves, need to change them when called here too\n",
    "- davletshinaztecan 0.20 bilingual not launched for all language pairs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9f8a02ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'O.10'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4330/2539504226.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0med\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_ed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             results_dict[language_family][cur_split][cur_model].update(\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"ED\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0med\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Normalized ED\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnorm_ed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"B2 F5\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'O.10'"
     ]
    }
   ],
   "source": [
    "results_dict = {language_family: {split: {\n",
    "    model: {} for model in models\n",
    "} for split in splits} for language_family in folders}\n",
    "\n",
    "for language_family in folders:\n",
    "    cur_model = \"baseline\"\n",
    "    cur_split = \"0.10\"\n",
    "    with open(os.path.join(copper_dir, raw_data_path, f\"analysis_{language_family}.txt\"), \"r\") as f: \n",
    "        for line in f:\n",
    "            if \"Language\" in line: continue\n",
    "            if \"--\" in line: continue\n",
    "            if line[0] == \")\":\n",
    "                cur_split, cur_model = line[1:].replace(\"\\n\", \"\").split(\" - \");\n",
    "                continue\n",
    "            lang, ed, norm_ed, f5 = \" \".join(line.split()).split()\n",
    "            results_dict[language_family][cur_split][cur_model].update(\n",
    "                {lang: {\"ED\": ed, \"Normalized ED\": norm_ed, \"B2 F5\": f5}}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d62c1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shared_bilingual', 'shared_multilingual', 'shared_statistical']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7e0fcf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abrahammonpa 0.10 {'ED': '0.459', 'Normalized ED': '0.088', 'B2 F5': '0.884'}\n",
      "abrahammonpa 0.10 {'ED': '1.150', 'Normalized ED': '0.223', 'B2 F5': '0.712'}\n",
      "abrahammonpa 0.10 {'ED': '1.041', 'Normalized ED': '0.194', 'B2 F5': '0.725'}\n",
      "abrahammonpa 0.10 {'ED': '0.372', 'Normalized ED': '0.069', 'B2 F5': '0.900'}\n",
      "abrahammonpa 0.50 {'ED': '1.486', 'Normalized ED': '0.281', 'B2 F5': '0.687'}\n",
      "abrahammonpa 0.50 {'ED': '3.483', 'Normalized ED': '0.657', 'B2 F5': '0.285'}\n",
      "abrahammonpa 0.50 {'ED': '3.694', 'Normalized ED': '0.675', 'B2 F5': '0.249'}\n",
      "abrahammonpa 0.50 {'ED': '0.961', 'Normalized ED': '0.168', 'B2 F5': '0.724'}\n",
      "allenbai baseline 0.10\n",
      "allenbai BiNMT 0.10\n",
      "allenbai MNMT 0.10\n",
      "allenbai SMT 0.10\n",
      "allenbai baseline 0.50\n",
      "allenbai BiNMT 0.50\n",
      "allenbai MNMT 0.50\n",
      "allenbai SMT 0.50\n",
      "backstromnorthernpakistan baseline 0.10\n",
      "backstromnorthernpakistan BiNMT 0.10\n",
      "backstromnorthernpakistan MNMT 0.10\n",
      "backstromnorthernpakistan SMT 0.10\n",
      "backstromnorthernpakistan baseline 0.50\n",
      "backstromnorthernpakistan BiNMT 0.50\n",
      "backstromnorthernpakistan MNMT 0.50\n",
      "backstromnorthernpakistan SMT 0.50\n",
      "castrosui baseline 0.10\n",
      "castrosui BiNMT 0.10\n",
      "castrosui MNMT 0.10\n",
      "castrosui SMT 0.10\n",
      "castrosui baseline 0.50\n",
      "castrosui BiNMT 0.50\n",
      "castrosui MNMT 0.50\n",
      "castrosui SMT 0.50\n",
      "davletshinaztecan baseline 0.10\n",
      "davletshinaztecan BiNMT 0.10\n",
      "davletshinaztecan MNMT 0.10\n",
      "davletshinaztecan SMT 0.10\n",
      "davletshinaztecan baseline 0.50\n",
      "davletshinaztecan BiNMT 0.50\n",
      "davletshinaztecan MNMT 0.50\n",
      "davletshinaztecan SMT 0.50\n",
      "felekesemitic baseline 0.10\n",
      "felekesemitic BiNMT 0.10\n",
      "felekesemitic MNMT 0.10\n",
      "felekesemitic SMT 0.10\n",
      "felekesemitic baseline 0.50\n",
      "felekesemitic BiNMT 0.50\n",
      "felekesemitic MNMT 0.50\n",
      "felekesemitic SMT 0.50\n",
      "hantganbangime baseline 0.10\n",
      "hantganbangime BiNMT 0.10\n",
      "hantganbangime MNMT 0.10\n",
      "hantganbangime SMT 0.10\n",
      "hantganbangime baseline 0.50\n",
      "hantganbangime BiNMT 0.50\n",
      "hantganbangime MNMT 0.50\n",
      "hantganbangime SMT 0.50\n",
      "hattorijaponic baseline 0.10\n",
      "hattorijaponic BiNMT 0.10\n",
      "hattorijaponic MNMT 0.10\n",
      "hattorijaponic SMT 0.10\n",
      "hattorijaponic baseline 0.50\n",
      "hattorijaponic BiNMT 0.50\n",
      "hattorijaponic MNMT 0.50\n",
      "hattorijaponic SMT 0.50\n",
      "listsamplesize baseline 0.10\n",
      "listsamplesize BiNMT 0.10\n",
      "listsamplesize MNMT 0.10\n",
      "listsamplesize SMT 0.10\n",
      "listsamplesize baseline 0.50\n",
      "listsamplesize BiNMT 0.50\n",
      "listsamplesize MNMT 0.50\n",
      "listsamplesize SMT 0.50\n",
      "mannburmish baseline 0.10\n",
      "mannburmish BiNMT 0.10\n",
      "mannburmish MNMT 0.10\n",
      "mannburmish SMT 0.10\n",
      "mannburmish baseline 0.50\n",
      "mannburmish BiNMT 0.50\n",
      "mannburmish MNMT 0.50\n",
      "mannburmish SMT 0.50\n"
     ]
    }
   ],
   "source": [
    "for k, v in results_dict.items():\n",
    "    for split in [\"0.10\", \"0.50\"]:\n",
    "        for model in models:\n",
    "            try:\n",
    "                print(k, split, v[split][model][\"TOTAL\"])\n",
    "            except KeyError:\n",
    "                print(k, model, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74a7524b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abrahammonpa 0.10 {'ED': '0.459', 'Normalized ED': '0.088', 'B2 F5': '0.884'}\n",
      "abrahammonpa 0.10 {'ED': '1.150', 'Normalized ED': '0.223', 'B2 F5': '0.712'}\n",
      "abrahammonpa 0.10 {'ED': '1.041', 'Normalized ED': '0.194', 'B2 F5': '0.725'}\n",
      "abrahammonpa 0.10 {'ED': '0.372', 'Normalized ED': '0.069', 'B2 F5': '0.900'}\n",
      "abrahammonpa 0.50 {'ED': '1.486', 'Normalized ED': '0.281', 'B2 F5': '0.687'}\n",
      "abrahammonpa 0.50 {'ED': '3.483', 'Normalized ED': '0.657', 'B2 F5': '0.285'}\n",
      "abrahammonpa 0.50 {'ED': '3.694', 'Normalized ED': '0.675', 'B2 F5': '0.249'}\n",
      "abrahammonpa 0.50 {'ED': '0.961', 'Normalized ED': '0.168', 'B2 F5': '0.724'}\n",
      "allenbai 0.10 {'ED': '0.882', 'Normalized ED': '0.288', 'B2 F5': '0.743'}\n",
      "allenbai BiNMT 0.10\n",
      "allenbai MNMT 0.10\n",
      "allenbai SMT 0.10\n",
      "allenbai 0.50 {'ED': '1.192', 'Normalized ED': '0.379', 'B2 F5': '0.637'}\n",
      "allenbai 0.50 {'ED': '1.273', 'Normalized ED': '0.406', 'B2 F5': '0.499'}\n",
      "allenbai 0.50 {'ED': '1.206', 'Normalized ED': '0.386', 'B2 F5': '0.519'}\n",
      "allenbai 0.50 {'ED': '0.856', 'Normalized ED': '0.276', 'B2 F5': '0.642'}\n",
      "backstromnorthernpakistan 0.10 {'ED': '1.389', 'Normalized ED': '0.288', 'B2 F5': '0.805'}\n",
      "backstromnorthernpakistan 0.10 {'ED': '1.520', 'Normalized ED': '0.310', 'B2 F5': '0.689'}\n",
      "backstromnorthernpakistan 0.10 {'ED': '1.949', 'Normalized ED': '0.402', 'B2 F5': '0.600'}\n",
      "backstromnorthernpakistan 0.10 {'ED': '0.497', 'Normalized ED': '0.102', 'B2 F5': '0.881'}\n",
      "backstromnorthernpakistan 0.50 {'ED': '1.596', 'Normalized ED': '0.354', 'B2 F5': '0.689'}\n",
      "backstromnorthernpakistan 0.50 {'ED': '2.956', 'Normalized ED': '0.656', 'B2 F5': '0.334'}\n",
      "backstromnorthernpakistan 0.50 {'ED': '3.283', 'Normalized ED': '0.694', 'B2 F5': '0.264'}\n",
      "backstromnorthernpakistan 0.50 {'ED': '0.643', 'Normalized ED': '0.142', 'B2 F5': '0.803'}\n",
      "castrosui 0.10 {'ED': '0.311', 'Normalized ED': '0.079', 'B2 F5': '0.920'}\n",
      "castrosui BiNMT 0.10\n",
      "castrosui MNMT 0.10\n",
      "castrosui SMT 0.10\n",
      "castrosui 0.50 {'ED': '0.692', 'Normalized ED': '0.175', 'B2 F5': '0.842'}\n",
      "castrosui BiNMT 0.50\n",
      "castrosui MNMT 0.50\n",
      "castrosui SMT 0.50\n",
      "davletshinaztecan 0.10 {'ED': '3.694', 'Normalized ED': '0.550', 'B2 F5': '0.529'}\n",
      "davletshinaztecan 0.10 {'ED': '3.231', 'Normalized ED': '0.565', 'B2 F5': '0.494'}\n",
      "davletshinaztecan 0.10 {'ED': '3.778', 'Normalized ED': '0.639', 'B2 F5': '0.445'}\n",
      "davletshinaztecan 0.10 {'ED': '1.833', 'Normalized ED': '0.296', 'B2 F5': '0.658'}\n",
      "davletshinaztecan 0.50 {'ED': '3.891', 'Normalized ED': '0.568', 'B2 F5': '0.476'}\n",
      "davletshinaztecan 0.50 {'ED': '4.695', 'Normalized ED': '0.753', 'B2 F5': '0.264'}\n",
      "davletshinaztecan 0.50 {'ED': '4.645', 'Normalized ED': '0.699', 'B2 F5': '0.274'}\n",
      "davletshinaztecan 0.50 {'ED': '2.229', 'Normalized ED': '0.362', 'B2 F5': '0.559'}\n",
      "felekesemitic 0.10 {'ED': '2.165', 'Normalized ED': '0.394', 'B2 F5': '0.596'}\n",
      "felekesemitic 0.10 {'ED': '3.569', 'Normalized ED': '0.679', 'B2 F5': '0.355'}\n",
      "felekesemitic MNMT 0.10\n",
      "felekesemitic 0.10 {'ED': '1.650', 'Normalized ED': '0.302', 'B2 F5': '0.649'}\n",
      "felekesemitic 0.50 {'ED': '5.074', 'Normalized ED': '0.842', 'B2 F5': '0.281'}\n",
      "felekesemitic BiNMT 0.50\n",
      "felekesemitic MNMT 0.50\n",
      "felekesemitic SMT 0.50\n",
      "hantganbangime 0.10 {'ED': '1.920', 'Normalized ED': '0.471', 'B2 F5': '0.521'}\n",
      "hantganbangime 0.10 {'ED': '2.656', 'Normalized ED': '0.664', 'B2 F5': '0.322'}\n",
      "hantganbangime MNMT 0.10\n",
      "hantganbangime 0.10 {'ED': '1.368', 'Normalized ED': '0.340', 'B2 F5': '0.579'}\n",
      "hantganbangime 0.50 {'ED': '3.614', 'Normalized ED': '0.838', 'B2 F5': '0.245'}\n",
      "hantganbangime BiNMT 0.50\n",
      "hantganbangime MNMT 0.50\n",
      "hantganbangime SMT 0.50\n",
      "hattorijaponic 0.10 {'ED': '1.739', 'Normalized ED': '0.346', 'B2 F5': '0.700'}\n",
      "hattorijaponic 0.10 {'ED': '1.214', 'Normalized ED': '0.265', 'B2 F5': '0.695'}\n",
      "hattorijaponic 0.10 {'ED': '1.893', 'Normalized ED': '0.385', 'B2 F5': '0.547'}\n",
      "hattorijaponic 0.10 {'ED': '0.786', 'Normalized ED': '0.171', 'B2 F5': '0.807'}\n",
      "hattorijaponic 0.50 {'ED': '2.208', 'Normalized ED': '0.423', 'B2 F5': '0.582'}\n",
      "hattorijaponic 0.50 {'ED': '3.599', 'Normalized ED': '0.750', 'B2 F5': '0.251'}\n",
      "hattorijaponic 0.50 {'ED': '3.332', 'Normalized ED': '0.643', 'B2 F5': '0.290'}\n",
      "hattorijaponic 0.50 {'ED': '1.536', 'Normalized ED': '0.290', 'B2 F5': '0.635'}\n",
      "listsamplesize 0.10 {'ED': '4.007', 'Normalized ED': '0.685', 'B2 F5': '0.401'}\n",
      "listsamplesize 0.10 {'ED': '2.708', 'Normalized ED': '0.560', 'B2 F5': '0.444'}\n",
      "listsamplesize 0.10 {'ED': '3.241', 'Normalized ED': '0.612', 'B2 F5': '0.401'}\n",
      "listsamplesize 0.10 {'ED': '2.379', 'Normalized ED': '0.473', 'B2 F5': '0.541'}\n",
      "listsamplesize 0.50 {'ED': '4.295', 'Normalized ED': '0.776', 'B2 F5': '0.327'}\n",
      "listsamplesize 0.50 {'ED': '3.300', 'Normalized ED': '0.703', 'B2 F5': '0.258'}\n",
      "listsamplesize 0.50 {'ED': '3.624', 'Normalized ED': '0.760', 'B2 F5': '0.225'}\n",
      "listsamplesize 0.50 {'ED': '2.621', 'Normalized ED': '0.494', 'B2 F5': '0.448'}\n",
      "mannburmish 0.10 {'ED': '2.729', 'Normalized ED': '0.666', 'B2 F5': '0.454'}\n",
      "mannburmish BiNMT 0.10\n",
      "mannburmish MNMT 0.10\n",
      "mannburmish 0.10 {'ED': '1.786', 'Normalized ED': '0.461', 'B2 F5': '0.523'}\n",
      "mannburmish 0.50 {'ED': '3.087', 'Normalized ED': '0.744', 'B2 F5': '0.288'}\n",
      "mannburmish BiNMT 0.50\n",
      "mannburmish MNMT 0.50\n",
      "mannburmish 0.50 {'ED': '2.517', 'Normalized ED': '0.634', 'B2 F5': '0.308'}\n"
     ]
    }
   ],
   "source": [
    "for k, v in results_dict.items():\n",
    "    for split in [\"0.10\", \"0.50\"]:\n",
    "        for model in models:\n",
    "            try:\n",
    "                print(k, split, v[split][model][\"TOTAL\"])\n",
    "            except KeyError:\n",
    "                print(k, model, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1accadaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
