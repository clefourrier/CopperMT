{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c005eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "840378de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26cedf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding module to sys path\n",
    "import sys\n",
    "sys.path.append(\"/home/cfourrie/documents/software/public/CopperMT/\")\n",
    "# RNN imports\n",
    "import pipeline\n",
    "import torch, numpy as np\n",
    "from fairseq import checkpoint_utils, data, options, tasks\n",
    "from pipeline.neural_translation.multilingual_rnns.multilingual_rnn import MultilingualRNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b414a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = \"inputs/raw_data/\" \n",
    "split_data_path = \"inputs/split_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b84c2256",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"abrahammonpa\", \"allenbai\", \"backstromnorthernpakistan\", \"castrosui\", \"davletshinaztecan\", \n",
    "           \"felekesemitic\", \"hantganbangime\", \"hattorijaponic\", \"listsamplesize\", \"mannburmish\"]\n",
    "splits = [\"0.10\", \"0.20\", \"0.30\", \"0.40\", \"0.50\"]\n",
    "train_name = \"training\"\n",
    "test_in_name = \"test\"\n",
    "test_out_name = \"solutions\"\n",
    "save_as = \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ddbf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_needed = True\n",
    "preprocessing_needed = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af2e2cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"/home/cfourrie/documents/software/public/CopperMT/workspace\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26858afc",
   "metadata": {},
   "source": [
    "## Convert data to usual format for our software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ef4e465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Antang', 'Banliang', 'Dujiang', 'Jiaoli', 'Jiarong', 'Jiuqian', 'Pandong', 'Renli', 'Sanjiang', 'Shuigen', 'Shuiwei', 'Shuiyao', 'Tangnian', 'Tangzhou', 'Tingpai', 'Zhonghe']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9547/3576547460.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mcur_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'COGID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcur_languages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_languages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_languages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"{l1}-{l2}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{l2}-{l1}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "# If you do not want to share embeddings, removed shared_ from the folder path\n",
    "for folder in folders:\n",
    "    for split in splits:\n",
    "        try: os.makedirs(os.path.join(split_data_path, f\"shared_{folder}\", split))\n",
    "        except FileExistsError: pass\n",
    "        # Training\n",
    "        cur_data = pd.read_csv(os.path.join(raw_data_path, folder, f\"training-{split}.tsv\"), sep=\"\\t\")\n",
    "        cur_languages = [c for c in cur_data.columns if c != 'COGID']\n",
    "        if folder == \"castrosui\":\n",
    "            cur_languages = [re.split('(?=[A-Z])', l)[1] for l in cur_languages]\n",
    "            cur_data.columns = ['COGID'] + cur_languages\n",
    "        for l1, l2 in itertools.combinations(cur_languages, 2):\n",
    "            for name in [f\"{l1}-{l2}\", f\"{l2}-{l1}\"]:\n",
    "                with open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"train.{name}.{l1}\"), \"w+\") as f1, \\\n",
    "                     open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"train.{name}.{l2}\"), \"w+\") as f2:\n",
    "                    for ix, row in cur_data[[l1, l2]].dropna().iterrows():\n",
    "                        f1.write(row[l1] + \"\\n\")\n",
    "                        f2.write(row[l2] + \"\\n\")\n",
    "        for l in cur_languages:\n",
    "            with open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"train.{l}-{l}.{l}\"), \"w+\") as f:\n",
    "                for ix, row in cur_data[[l]].dropna().iterrows():\n",
    "                    f.write(row[l] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6855a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of test and validation sets \n",
    "# If you do not want to share embeddings, removed shared_ from the folder path\n",
    "for folder in folders:\n",
    "    for split in splits:\n",
    "        try: os.makedirs(os.path.join(split_data_path, f\"shared_{folder}\", split))\n",
    "        except FileExistsError: pass\n",
    "        # Training\n",
    "        cur_data = pd.read_csv(os.path.join(raw_data_path, folder, f\"test-{split}.tsv\"), sep=\"\\t\")\n",
    "        cur_languages = [c for c in cur_data.columns if c != 'COGID']\n",
    "        if folder == \"castrosui\":\n",
    "            cur_languages = [re.split('(?=[A-Z])', l)[1] for l in cur_languages]\n",
    "            cur_data.columns = ['COGID'] + cur_languages\n",
    "\n",
    "        for l1, l2 in itertools.combinations(cur_languages, 2):\n",
    "            with open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"test.{l1}-{l2}.{l1}\"), \"w+\") as f1_test, \\\n",
    "                 open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"test.{l2}-{l1}.{l2}\"), \"w+\") as f2_test, \\\n",
    "                 open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"valid.{l1}-{l2}.{l1}\"), \"w+\") as f1_val, \\\n",
    "                 open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"valid.{l1}-{l2}.{l2}\"), \"w+\") as f2_val, \\\n",
    "                 open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"valid.{l2}-{l1}.{l1}\"), \"w+\") as f1_val_r, \\\n",
    "                 open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"valid.{l2}-{l1}.{l2}\"), \"w+\") as f2_val_r:\n",
    "                    for ix, row in cur_data[[l1, l2]].dropna().iterrows():\n",
    "                        if row[l1] != \"?\" and row[l2] != \"?\":                                \n",
    "                            f1_val.write(row[l1] + \"\\n\")                            \n",
    "                            f1_val_r.write(row[l1] + \"\\n\")                            \n",
    "                            f2_val.write(row[l2] + \"\\n\")                            \n",
    "                            f2_val_r.write(row[l2] + \"\\n\")\n",
    "                        if row[l1] == \"?\" and row[l2] != \"?\":\n",
    "                            f2_test.write(row[l2] + \"\\n\")\n",
    "                        if row[l2] == \"?\" and row[l1] != \"?\":\n",
    "                            f1_test.write(row[l1] + \"\\n\")\n",
    "\n",
    "        for l in cur_languages:\n",
    "            with open(os.path.join(split_data_path, f\"shared_{folder}\", split, f\"valid.{l}-{l}.{l}\"), \"w+\") as f:\n",
    "                for ix, row in cur_data[[l]].dropna().iterrows():\n",
    "                    if row[l] != \"?\":\n",
    "                        f.write(row[l] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e05e28",
   "metadata": {},
   "source": [
    "## Generating configuration files contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d7b2426",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if preprocessing_needed:\n",
    "    folder = folders[3]\n",
    "    cur_data = pd.read_csv(os.path.join(raw_data_path, folder, f\"cognates.tsv\"), sep=\"\\t\")\n",
    "    cur_languages = [c for c in cur_data.columns if c != 'COGID']\n",
    "    if folder == \"castrosui\":\n",
    "        cur_languages = [re.split('(?=[A-Z])', l)[1] for l in cur_languages]\n",
    "\n",
    "    print('PROJ_DIR=\"/home/cfourrie/documents/software/CopperMT\"')\n",
    "    print('MOSES_DIR=\"${PROJ_DIR}/submodules\"')\n",
    "    print()\n",
    "    print('WK_DIR=\"${PROJ_DIR}/workspace/' + folder + '\"')\n",
    "    print('INPUTS_DIR=\"${PROJ_DIR}/inputs\"')\n",
    "    print()\n",
    "    print(f'DATA_NAME=\"{folder}\"')\n",
    "\n",
    "    print(f'langs_bi=\"{\",\".join(\"-\".join(l) for l in itertools.product(cur_languages, cur_languages))}\"')\n",
    "    print(f'langs=\"{\",\".join(cur_languages)}\"')\n",
    "    print(f'langs_shared=\"{\"-\".join(cur_languages)}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf8738",
   "metadata": {},
   "source": [
    "Step: 1h30 - In the pipeline folder, execute:\n",
    "\n",
    "`\n",
    "bash data_preprocess.sh parameters_abrahammonpa.cfg\n",
    "bash data_preprocess.sh parameters_allenbai.cfg\n",
    "bash data_preprocess.sh parameters_backstromnorthernpakistan.cfg\n",
    "bash data_preprocess.sh parameters_castrosui.cfg\n",
    "bash data_preprocess.sh parameters_davletshinaztecan.cfg\n",
    "bash data_preprocess.sh parameters_felekesemitic.cfg\n",
    "bash data_preprocess.sh parameters_hantganbangime.cfg\n",
    "bash data_preprocess.sh parameters_hattorijaponic.cfg\n",
    "bash data_preprocess.sh parameters_listsamplesize.cfg\n",
    "bash data_preprocess.sh parameters_mannburmish.cfg\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a787af",
   "metadata": {},
   "source": [
    "## Training models\n",
    "\n",
    "In the pipeline folder, train bilingual neural models with:\n",
    "\n",
    "`\n",
    "bash main_nmt_bilingual_full.sh parameters_abrahammonpa.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_allenbai.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_backstromnorthernpakistan.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_castrosui.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_davletshinaztecan.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_felekesemitic.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_hantganbangime.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_hattorijaponic.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_listsamplesize.cfg\n",
    "bash main_nmt_bilingual_full.sh parameters_mannburmish.cfg\n",
    "`\n",
    "\n",
    "In the pipeline folder, train multilingual neural models with:\n",
    "\n",
    "`\n",
    "bash main_nmt_multilingual_full.sh parameters_abrahammonpa.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_allenbai.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_backstromnorthernpakistan.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_castrosui.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_davletshinaztecan.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_felekesemitic.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_hantganbangime.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_hattorijaponic.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_listsamplesize.cfg\n",
    "bash main_nmt_multilingual_full.sh parameters_mannburmish.cfg\n",
    "`\n",
    "\n",
    "\n",
    "In the pipeline folder, train bilingual statistical models with:\n",
    "\n",
    "`\n",
    "bash main_smt_full.sh parameters_abrahammonpa.cfg\n",
    "bash main_smt_full.sh parameters_allenbai.cfg\n",
    "bash main_smt_full.sh parameters_backstromnorthernpakistan.cfg\n",
    "bash main_smt_full.sh parameters_castrosui.cfg\n",
    "bash main_smt_full.sh parameters_davletshinaztecan.cfg\n",
    "bash main_smt_full.sh parameters_felekesemitic.cfg\n",
    "bash main_smt_full.sh parameters_hantganbangime.cfg\n",
    "bash main_smt_full.sh parameters_hattorijaponic.cfg\n",
    "bash main_smt_full.sh parameters_listsamplesize.cfg\n",
    "bash main_smt_full.sh parameters_mannburmish.cfg\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4299500",
   "metadata": {},
   "source": [
    "# Choosing best answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61b9a42",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cdc029ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sublist(ls1, ls2):\n",
    "    ''' From https://stackoverflow.com/questions/35964155/checking-if-list-is-a-sublist, with some edits '''\n",
    "    def get_all_in(one, another):\n",
    "        for element in one:\n",
    "            if element in another:\n",
    "                yield element\n",
    "    ix = -1         \n",
    "    for ix, (x1, x2) in enumerate(zip(ls1, get_all_in(ls2, ls1))):\n",
    "        if x1 != x2:\n",
    "            return False\n",
    "\n",
    "    return not (ix == -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ad2243ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neural_bleu_predictions(path, l_in, l_out, n_best):\n",
    "    # Storage\n",
    "    source = []\n",
    "    target = []\n",
    "    prediction = []\n",
    "    confidence = []\n",
    "    cur_prediction = []\n",
    "    cur_confidence = []\n",
    "    indices = []\n",
    "    with open(\n",
    "            f'{path}/bleu/bleu_checkpoint_best_{l_in}-{l_out}.{l_out}', 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            line = line.split(\"\\t\")\n",
    "            # Actual source\n",
    "            if \"S-\" in line[0]:\n",
    "                word = line[1].strip(' ').split()\n",
    "                source.append(word)\n",
    "                indices.append(line[0])\n",
    "                # We reinitialize the cur_prediction list\n",
    "                if len(cur_prediction) > 0:\n",
    "                    prediction.append(cur_prediction)\n",
    "                    confidence.append(cur_confidence)\n",
    "                    cur_prediction = []\n",
    "                    cur_confidence = []\n",
    "            # Actual target\n",
    "            if \"T-\" in line[0]:\n",
    "                word = line[1].strip(' ').split()\n",
    "                target.append(word)\n",
    "            # Hypothesis\n",
    "            if \"H-\" in line[0] and len(cur_prediction) < n_best:\n",
    "                word = line[2].strip(' ').split()\n",
    "                cur_prediction.append(word)\n",
    "                cur_confidence.append(math.exp(float(line[1])))\n",
    "        prediction.append(cur_prediction)\n",
    "        confidence.append(cur_confidence)\n",
    "        prediction = [[bor[n] for bor in prediction] for n in range(n_best)]\n",
    "\n",
    "    return source, target, prediction, confidence, indices\n",
    "\n",
    "\n",
    "def get_statistical_bleu_predictions(path_data, path, l_in, l_out, n_best, cur_n_best):\n",
    "    target = []\n",
    "    try:\n",
    "        with open(f'{path_data}/test.{l_in}_{l_out}.{l_out}', 'r') as file:\n",
    "            for i, line in enumerate(file):\n",
    "                target.append(line.split())\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    source = []\n",
    "    with open(f'{path_data}/test.{l_in}_{l_out}.{l_in}', 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            source.append(line.split())\n",
    "\n",
    "    prediction = []\n",
    "    confidence = []\n",
    "    cur_ix = -1\n",
    "    cur_prediction = []\n",
    "    cur_confidence = []\n",
    "    indices = []\n",
    "    with open(f'{path}/{l_in}_{l_out}/out/'\n",
    "              f'test.{l_in}_{l_out}_nbest_{str(n_best)}.{l_out}', 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            line = line.split(\"|||\")\n",
    "            ix = int(line[0])\n",
    "            word = line[1].strip(' ').split()\n",
    "\n",
    "            if cur_ix != ix:\n",
    "                if cur_ix != -1:\n",
    "                    while len(cur_prediction) < cur_n_best:\n",
    "                        cur_prediction.append(cur_prediction[-1])\n",
    "                        cur_confidence.append(cur_confidence[-1])\n",
    "                        indices.append(cur_ix)\n",
    "                    prediction.append(cur_prediction)\n",
    "                    confidence.append(cur_confidence)\n",
    "                cur_prediction = [word]\n",
    "                cur_confidence = [math.exp(float(line[-1]))]\n",
    "                cur_ix = ix\n",
    "            else:\n",
    "                cur_prediction.append(word)\n",
    "                cur_confidence.append(math.exp(float(line[-1])))\n",
    "        # Management of last prediction\n",
    "        while len(cur_prediction) < cur_n_best:\n",
    "            cur_prediction.append(cur_prediction[-1])\n",
    "            cur_confidence.append(cur_confidence[-1])\n",
    "        prediction.append(cur_prediction)\n",
    "        confidence.append(cur_confidence)\n",
    "\n",
    "    prediction = [[bor[n] for bor in prediction] for n in range(cur_n_best)]\n",
    "\n",
    "    return source, target, prediction, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb0245d",
   "metadata": {},
   "source": [
    "## Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fd9e0e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_data = folders[0]\n",
    "split = \"0.10\"\n",
    "\n",
    "df = pd.read_csv(os.path.join(raw_data_path, cur_data, f\"cognates.tsv\"), sep=\"\\t\")\n",
    "langs = [c for c in df.columns if c != 'COGID']\n",
    "models = [\"shared_bilingual\", \"shared_multilingual\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b1fa3f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e8ef3f45",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/cfourrie/documents/software/public/CopperMT/workspace/abrahammonpa/shared_multilingual/0.20/bleu/bleu_checkpoint_best_MonpaDirang-MonpaBalemu.MonpaBalemu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9547/3496991321.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                             f\"{models_dir}/{cur_data}/{model}/{lang_in}-{lang_out}/{split}\", lang_in, lang_out, 10)\n\u001b[1;32m     17\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                         all_sources, _, all_predictions, all_confidences, all_indices = get_neural_bleu_predictions(\n\u001b[0m\u001b[1;32m     19\u001b[0m                             f\"{models_dir}/{cur_data}/{model}/{split}\", lang_in, lang_out, 10)\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9547/3373452722.py\u001b[0m in \u001b[0;36mget_neural_bleu_predictions\u001b[0;34m(path, l_in, l_out, n_best)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcur_confidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     with open(\n\u001b[0m\u001b[1;32m     11\u001b[0m             f'{path}/bleu/bleu_checkpoint_best_{l_in}-{l_out}.{l_out}', 'r') as file:\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/cfourrie/documents/software/public/CopperMT/workspace/abrahammonpa/shared_multilingual/0.20/bleu/bleu_checkpoint_best_MonpaDirang-MonpaBalemu.MonpaBalemu'"
     ]
    }
   ],
   "source": [
    "for cur_data in folders:\n",
    "    for split in splits:\n",
    "        df = pd.read_csv(os.path.join(raw_data_path, cur_data, f\"cognates.tsv\"), sep=\"\\t\")\n",
    "        langs = [c for c in df.columns if c != 'COGID']\n",
    "\n",
    "        # Read results\n",
    "        results_grouped = {model: {lang: defaultdict(dict) for lang in langs} for model in models}\n",
    "        results_by_lang = {model: {lang: defaultdict(dict) for lang in langs} for model in models}\n",
    "\n",
    "        for model in models:\n",
    "            for lang_out in langs:\n",
    "                for lang_in in langs:\n",
    "                    if lang_in == lang_out: continue\n",
    "                    if model == \"shared_bilingual\":\n",
    "                        all_sources, _, all_predictions, all_confidences, all_indices = get_neural_bleu_predictions(\n",
    "                            f\"{models_dir}/{cur_data}/{model}/{lang_in}-{lang_out}/{split}\", lang_in, lang_out, 10)\n",
    "                    else:\n",
    "                        all_sources, _, all_predictions, all_confidences, all_indices = get_neural_bleu_predictions(\n",
    "                            f\"{models_dir}/{cur_data}/{model}/{split}\", lang_in, lang_out, 10)\n",
    "\n",
    "                    for ix, (source, index) in enumerate(zip(all_sources, all_indices)):\n",
    "                        predictions = [\" \".join(all_predictions[n_best][ix]) for n_best in range(10)]\n",
    "                        confidences = all_confidences[ix]\n",
    "                        results_grouped[model][lang_out][index].update(\n",
    "                            {f\"{lang_in}_source\": \" \".join(source),\n",
    "                             lang_in: sorted([(p, c) for p, c in zip(predictions, confidences)])}\n",
    "                        ) \n",
    "                        results_by_lang[model][lang_out][lang_in].update(\n",
    "                             {\" \".join(source): sorted([(p, c) for p, c in zip(predictions, confidences)])}\n",
    "                        ) \n",
    "\n",
    "            # Reorder according to initial file\n",
    "            test_df = pd.read_csv(os.path.join(raw_data_path, cur_data, f\"test-{split}.tsv\"), sep=\"\\t\")\n",
    "            final_results = defaultdict(list)\n",
    "            for ix, (_, row) in enumerate(test_df.iterrows()):\n",
    "                row_dict = dict(row)\n",
    "                final_results[\"COGID\"].append(row_dict.pop(\"COGID\"))\n",
    "                lang_out = [k for k, v in row_dict.items() if v == \"?\"][0]\n",
    "                row_results = {}\n",
    "                # Compute all predictions\n",
    "                for lang, val in row_dict.items():\n",
    "                    if lang == lang_out: continue\n",
    "                    #print(val, results_by_key[model][lang_out][val])\n",
    "                    try:\n",
    "                        row_results[lang] = results_by_lang[model][lang_out][lang][val]\n",
    "                    except KeyError: # some chars are only present in test, and encoded as unk\n",
    "                        # We extract possible keys \n",
    "                        keys_with_unk = [v for v in results_by_lang[model][lang_out][lang].keys() \n",
    "                                         if \"<unk>\" in v and len(v.split(\" \")) == len(val.split(\" \"))]\n",
    "                        possible_keys = []\n",
    "                        for key in keys_with_unk:\n",
    "                            if sublist([phone for phone in key.split(\" \") if phone != \"<unk>\"], val.split(\" \")):\n",
    "                                possible_keys.append(key)\n",
    "\n",
    "                        if len(possible_keys) > 1:\n",
    "                            raise Exception(\"Problem! Several plausible keys!\", val, possible_keys)\n",
    "                        elif len(possible_keys) == 0:\n",
    "                            raise Exception(\"Problem! No plausible key!\", val)\n",
    "                        else:\n",
    "                            row_results[lang] = results_by_lang[model][lang_out][lang][possible_keys[0]]\n",
    "\n",
    "                    final_results[lang].append(\"\")\n",
    "\n",
    "                # Rank best prediction\n",
    "                predictions_scores = defaultdict(int)\n",
    "                predictions_counts = defaultdict(int)\n",
    "                for lang_res in row_results.values():\n",
    "                    for pred, score in lang_res:\n",
    "                        predictions_scores[pred] += score\n",
    "                        predictions_counts[pred] += 1\n",
    "                best_prediction = [k for k, v in predictions_scores.items() if v == max(predictions_scores.values())]\n",
    "                if best_prediction:\n",
    "                    best_prediction = best_prediction[0]\n",
    "                else:\n",
    "                    best_prediction = \"\"\n",
    "                # Save\n",
    "                final_results[lang_out].append(best_prediction)\n",
    "\n",
    "            # Store best prediction\n",
    "            with open(os.path.join(raw_data_path, cur_data, f\"results-{model}-{split}.tsv\"), \"w+\") as f:\n",
    "                f.write(\"COGID\\t\" + \"\\t\".join(langs) + \"\\n\")\n",
    "                for ix in range(len(final_results[\"COGID\"])):\n",
    "                    f.write(\"\\t\".join([final_results[label][ix] if final_results[label][ix] else \"\" for label in [\"COGID\"] + langs]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ed0e4",
   "metadata": {},
   "source": [
    "```bash\n",
    "source ~/Desktop/SIGTYP2022/venv/bin/activate\n",
    "# Baseline\n",
    "st2022 --compare --prediction-file=abrahammonpa/result-0.10.tsv --solution-file=abrahammonpa/solutions-0.10.tsv\n",
    "# My predictions\n",
    "st2022 --compare --prediction-file=abrahammonpa/results-shared_bilingual-0.10.tsv --solution-file=abrahammonpa/solutions-0.10.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d583696f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
